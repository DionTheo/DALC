{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"BERT4DALC.ipynb","provenance":[{"file_id":"18p-Z2LZvHhMwkLFrU87sPw7vwnb07Q4M","timestamp":1611416037973},{"file_id":"1xA9oI_SZ27_AjEcMP2-bnNaWQE0t43KY","timestamp":1589541764133}],"collapsed_sections":["Blna25lRGj4y"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yLh8kkL3GEDq"},"source":["# Setting up the GPU for the program"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QIHzH3jCERM5","executionInfo":{"status":"ok","timestamp":1611777012235,"user_tz":-60,"elapsed":2634,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}},"outputId":"3457a6f0-8cfd-4dcf-9b01-c9603b5be0a6"},"source":["import tensorflow as tf\n","\n","\n","# Get the GPU device name:\n","device_name = tf.test.gpu_device_name()\n","if device_name == '/device:GPU:0':\n","  print('Found GPU at: {}'.format(device_name))\n","else:\n","  raise SystemError('GPU device not found')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hk2d_LqzFZvn","executionInfo":{"status":"ok","timestamp":1611777012443,"user_tz":-60,"elapsed":2791,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}},"outputId":"5058c2cc-8dae-4005-cddf-d0551d5e4e62"},"source":["import torch\n","\n","if torch.cuda.is_available():\n","\n","  device = torch.device('cuda')\n","  print('There are %d GPU(s) available' % torch.cuda.device_count())\n","  print('This program is using GPU:', torch.cuda.get_device_name(0))\n","\n","else:\n","  print('No GPU available, using CPU instead.')\n","  device = torch.device('cpu')\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available\n","This program is using GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sDsYumcyGKYl"},"source":["Installing the Hugging Face Library and the emoji library\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bT2_iepjGVsM","executionInfo":{"status":"ok","timestamp":1611777017059,"user_tz":-60,"elapsed":7330,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}},"outputId":"ea634c3e-999b-4652-c296-cb7b4111f390"},"source":["!pip install transformers\n","!pip install emoji"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.2)\n","Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (1.2.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Blna25lRGj4y"},"source":["# Loading in the annotated Abuse Dataset\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p2cXE3VtXqwv","executionInfo":{"status":"ok","timestamp":1611777017059,"user_tz":-60,"elapsed":7288,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}},"outputId":"586b8a4c-1b65-4325-bb39-4cb815ba2e05"},"source":["# Run this cell to mount your Google Drive.\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xaVhqYE7HppZ"},"source":["# Data inspection\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pac5zDwdH4ge","executionInfo":{"status":"ok","timestamp":1611777017060,"user_tz":-60,"elapsed":7247,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}},"outputId":"8702dbdd-42fb-44a9-f2aa-0b64045476d6"},"source":["import pandas as pd\n","import emoji\n","import re\n","from transformers import BertTokenizer\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","import numpy as np\n","\n","\n","\n","path = '/content/drive/MyDrive/DALC/'\n","\n","# manually curate split\n","#train = pd.read_csv(path + 'dalc_v2_train.csv', delimiter='\\t', header=0,  names=['id', 'text', 'user', 'offensive',\n","#                                                                       'abusive', 'target',\n","#                                                                       'source', 'user.description'])\n","#\n","#dev = pd.read_csv(path + 'dalc_v2_dev.csv', delimiter='\\t', header=0,  names=['id', 'text', \n","#                                                                   'user', 'offensive',\n","#                                                                   'abusive', 'target',\n","#                                                                   'source', 'user.description'])\n","#\n","\n","#test = pd.read_csv(path + 'dalc_v2_test.csv', delimiter='\\t', header=0,  names=['id', 'text',\n","#                                                        'user', 'offensive',\n","#                                                        'abusive', 'target',\n","#                                                        'source', 'user.description'])\n","\n","# random split\n","train = pd.read_csv(path + 'dalc_v2_train_random.csv', delimiter='\\t', header=0,  names=['id', 'text', 'user', 'offensive',\n","                                                                       'abusive', 'target',\n","                                                                       'source', 'user.description'])\n","\n","dev = pd.read_csv(path + 'dalc_v2_dev_random.csv', delimiter='\\t', header=0,  names=['id', 'text', \n","                                                                   'user', 'offensive',\n","                                                                   'abusive', 'target',\n","                                                                   'source', 'user.description'])\n","\n","\n","test = pd.read_csv(path + 'dalc_v2_test_random.csv', delimiter='\\t', header=0,  names=['id', 'text',\n","                                                        'user', 'offensive',\n","                                                        'abusive', 'target',\n","                                                        'source', 'user.description'])\n","\n","\n","\n","print('Number of training sentences: {:,}\\n'.format(train.shape[0]))\n","print('Number of dev sentences: {:,}\\n'.format(dev.shape[0]))\n","print('Number of test sentences: {:,}\\n'.format(test.shape[0]))\n","print(train[['text', 'abusive', 'target']].head())"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Number of training sentences: 5,706\n","\n","Number of dev sentences: 549\n","\n","Number of test sentences: 1,901\n","\n","                                                text   abusive      target\n","0  Misschien de ideale assistente van Francesca V...       NOT         NaN\n","1  en er stond een guy tegenover mij mr ik moest ...  EXPLICIT  INDIVIDUAL\n","2  #NL_actueel Uitgewezen asielzoekers steeds vak...       NOT         NaN\n","3  @beatsbyarti @mrouwen met wat je hier nu zegt,...  EXPLICIT  INDIVIDUAL\n","4  Waar komt al dat vertrouwen in @SanderDekker t...       NOT         NaN\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M4vJUqZII0vB"},"source":["The sentence column will be used during the classifcation task.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DkOeR-4bJO6n","executionInfo":{"status":"ok","timestamp":1611777017061,"user_tz":-60,"elapsed":7170,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}},"outputId":"41c1bc96-bccf-48dc-fb52-feecf1b5a75f"},"source":["# Placing the sentence and label columns into a list of values\n","\n","train_labels = train.abusive.values\n","dev_labels = dev.abusive.values\n","test_labels = test.abusive.values\n","\n","# Reformatting the labels binary, 0 = not abusive, 1 = abusive\n","def reformat_labels(labels):\n","  b_labels = []\n","  abusive_count = 0\n","  not_count = 0\n","  explicit_count = 0\n","  implicit_count = 0\n","  \n","  for label in labels:\n","    # binary\n","    #if label == 'NOT':\n","    #  not_count += 1\n","    #  b_labels.append(0)\n","    #else:\n","    #  abusive_count += 1\n","    #  b_labels.append(1)\n","\n","    # ternary\n","    if label == 'NOT':\n","      not_count += 1\n","      b_labels.append(0)\n","    elif label == 'EXPLICIT':\n","      explicit_count += 1\n","      b_labels.append(1)\n","    else:\n","      abusive_count += 1\n","      b_labels.append(2)\n","\n","\n","#  return b_labels, abusive_count, not_count # binary\n","  return b_labels, explicit_count, implicit_count, not_count # ternary\n","\n","# binary\n","#print('Formatting train labels:')\n","#train_labels, ab_count, not_count = reformat_labels(train_labels)\n","#print('Abusive: {} | Not: {}'.format(ab_count, not_count))\n","#print('Formatting dev labels:')\n","#dev_labels, ab_count, not_count = reformat_labels(dev_labels)\n","#print('Abusive: {} | Not: {}'.format(ab_count, not_count))\n","#print('Formatting test labels:')\n","#test_labels, ab_count, not_count = reformat_labels(test_labels)\n","#print('Abusive: {} | Not: {}'.format(ab_count, not_count))\n","\n","# ternary\n","print('Formatting train labels:')\n","train_labels, ab_count_exp, ab_count_imp, not_count = reformat_labels(train_labels)\n","print('Exp: {} | Imp: {} | Not: {}'.format(ab_count_exp, ab_count_imp, not_count))\n","print('Formatting dev labels:')\n","dev_labels, ab_count_exp, ab_count_imp, not_count = reformat_labels(dev_labels)\n","print('Exp: {} | Imp: {} | Not: {}'.format(ab_count_exp, ab_count_imp, not_count))\n","print('Formatting test labels:')\n","test_labels, ab_count_exp, ab_count_imp, not_count = reformat_labels(test_labels)\n","print('Exp: {} | Imp: {} | Not: {}'.format(ab_count_exp, ab_count_imp, not_count))\n","\n","\n","\n","#print('Checking if labels were formatted correctly:')\n","#print(train_labels)\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Formatting train labels:\n","Exp: 738 | Imp: 0 | Not: 4564\n","Formatting dev labels:\n","Exp: 77 | Imp: 0 | Not: 439\n","Formatting test labels:\n","Exp: 412 | Imp: 0 | Not: 1264\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HBXf-TB6Q-YW"},"source":["# Pre-processing steps\n","For pre-processing we decided to change each user mention (@folkert) to Name, change links to URL, remove numbers and emojis.\n"]},{"cell_type":"code","metadata":{"id":"FH1rWtFxROpz","executionInfo":{"status":"ok","timestamp":1611777021627,"user_tz":-60,"elapsed":11683,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}}},"source":["def clean_samples(data):\n","\n","  new_samples = []\n","  #print(data.head())\n","\n","  content = list(data['text'].values)\n","  for tweet_message in content:\n","      tweet_message = re.sub(r'https.*[^ ]', 'URL', tweet_message)\n","      tweet_message = re.sub(r'http.*[^ ]', 'URL', tweet_message)\n","      tweet_message = re.sub(r'@([^ ]*)', '@USER', tweet_message)\n","      tweet_message = emoji.demojize(tweet_message)\n","      tweet_message = re.sub(r'(:.*?:)', r' \\1 ', tweet_message)\n","      tweet_message = re.sub(' +', ' ', tweet_message)\n","      new_samples.append(tweet_message)\n","\n","  return new_samples\n","\n","# Formatting other dataframes as well\n","train_clean = clean_samples(train) # list\n","dev_clean = clean_samples(dev) # list\n","test_clean = clean_samples(test) # list\n","\n","#print(dev_clean[0:3])\n","\n","\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1GbUv7N4JdIA"},"source":["# Tokenization & Input Formatting"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lg7Tkvo0JjgK","executionInfo":{"status":"ok","timestamp":1611777021892,"user_tz":-60,"elapsed":11930,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}},"outputId":"2d6ce067-b991-400e-e2a2-8c9298c83997"},"source":["print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('wietsedv/bert-base-dutch-cased')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Loading BERT tokenizer...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1oVS-dBKJ-tS","executionInfo":{"status":"ok","timestamp":1611777021893,"user_tz":-60,"elapsed":11897,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}},"outputId":"4f1d76a0-f0a0-4ab9-ea79-99333caf17ca"},"source":["# Inspecting the output of the tokenizer\n","\n","# Original Sentence\n","print('Original: ', train_clean[0])\n","# Sentence split into tokens\n","print('Tokenized: ', tokenizer.tokenize(train_clean[0]))\n","# Sentence mapped to token ids\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_clean[0])))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Original:  Misschien de ideale assistente van Francesca Vanthielen in het #communicatie-team van #unia? Werken aan de campagne \"#racisme en #klimaat\"? URL\n","Tokenized:  ['Miss', '##chi', '##en', 'de', 'ideale', 'assistent', '##e', 'van', 'Francesca', 'Van', '##thi', '##elen', 'in', 'het', '[UNK]', 'communicatie', '-', 'team', 'van', '[UNK]', 'uni', '##a', '?', 'Werken', 'aan', 'de', 'campagne', '\"', '[UNK]', 'racisme', 'en', '[UNK]', 'klimaat', '\"', '?', 'U', '##RL']\n","Token IDs:  [4900, 24589, 25108, 10537, 13566, 8605, 117, 20722, 2717, 7222, 28981, 25070, 13644, 13261, 0, 10250, 12, 19884, 20722, 0, 20651, 113, 27, 7593, 7862, 10537, 10052, 6, 0, 17939, 11281, 0, 14357, 6, 27, 47, 23618]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZhmRQGRwKT37","executionInfo":{"status":"ok","timestamp":1611777024284,"user_tz":-60,"elapsed":14225,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}},"outputId":"f31acafc-baf8-4b3b-d7fa-07e369f2ce80"},"source":["max_len = 0\n","length = []\n","for sent in train_clean:\n","  \n","  # Tokenize the text and adding [CLS] and [SEP] tokens\n","  input_ids = tokenizer.encode(sent, add_special_tokens = True)\n","\n","  # Update the maximum sentence length\n","  max_len = max(max_len, len(input_ids))\n","  length.append(len(input_ids))\n","\n","avg_len = sum(length) // len(length)\n","\n","print('Max sentence length: ', max_len)\n","print('Average sentence length: ', avg_len)\n","print(length)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n"],"name":"stderr"},{"output_type":"stream","text":["Max sentence length:  528\n","Average sentence length:  36\n","[39, 73, 18, 61, 16, 22, 18, 17, 16, 31, 22, 15, 22, 83, 29, 41, 33, 21, 19, 17, 34, 34, 72, 30, 16, 26, 24, 22, 49, 28, 20, 48, 23, 49, 27, 76, 58, 23, 14, 39, 20, 18, 87, 8, 31, 31, 13, 77, 17, 32, 28, 11, 84, 13, 28, 60, 36, 4, 91, 14, 26, 74, 37, 17, 46, 29, 37, 26, 17, 22, 75, 30, 51, 19, 72, 52, 72, 14, 37, 12, 36, 7, 34, 63, 38, 33, 37, 16, 56, 33, 25, 27, 22, 25, 23, 33, 70, 32, 23, 20, 23, 30, 31, 17, 19, 20, 15, 48, 31, 16, 10, 25, 16, 22, 69, 16, 13, 63, 32, 60, 117, 66, 43, 37, 47, 24, 35, 23, 26, 29, 27, 41, 25, 21, 83, 18, 19, 29, 23, 17, 19, 49, 40, 17, 63, 58, 23, 31, 77, 31, 33, 170, 24, 72, 45, 15, 40, 29, 61, 32, 38, 40, 64, 11, 36, 51, 31, 38, 56, 22, 75, 65, 53, 7, 33, 19, 45, 60, 33, 68, 55, 54, 32, 17, 25, 45, 14, 36, 35, 22, 23, 24, 63, 21, 56, 39, 5, 34, 45, 44, 22, 23, 72, 24, 38, 46, 59, 69, 53, 17, 12, 25, 39, 31, 25, 17, 49, 16, 28, 24, 62, 25, 33, 39, 22, 11, 25, 47, 11, 96, 32, 50, 32, 54, 35, 24, 30, 42, 40, 27, 69, 11, 7, 21, 18, 84, 97, 95, 16, 33, 13, 65, 22, 69, 68, 21, 92, 10, 15, 27, 38, 20, 25, 34, 46, 13, 19, 44, 21, 24, 19, 41, 44, 15, 23, 20, 23, 47, 12, 33, 25, 44, 7, 27, 47, 26, 39, 13, 36, 31, 13, 14, 11, 33, 26, 23, 74, 17, 43, 50, 14, 22, 14, 57, 88, 19, 9, 33, 14, 23, 21, 30, 11, 31, 32, 37, 24, 28, 4, 26, 42, 20, 10, 65, 50, 10, 71, 34, 23, 38, 9, 42, 70, 29, 19, 75, 20, 18, 21, 21, 23, 5, 18, 31, 39, 23, 56, 15, 49, 66, 64, 25, 37, 93, 40, 14, 55, 69, 76, 59, 31, 25, 14, 52, 30, 38, 77, 18, 23, 15, 13, 48, 34, 24, 44, 85, 29, 23, 22, 32, 17, 25, 32, 32, 48, 31, 21, 61, 82, 50, 20, 19, 17, 36, 28, 19, 66, 76, 61, 21, 37, 21, 26, 82, 88, 19, 31, 69, 43, 22, 19, 13, 57, 15, 57, 22, 34, 19, 33, 18, 26, 43, 61, 10, 28, 104, 69, 71, 59, 53, 26, 58, 34, 37, 50, 7, 23, 14, 17, 41, 78, 22, 40, 18, 25, 57, 30, 60, 23, 30, 11, 26, 34, 7, 69, 34, 19, 20, 26, 26, 55, 62, 17, 29, 25, 48, 40, 24, 71, 19, 37, 30, 63, 28, 35, 35, 10, 107, 14, 20, 66, 29, 67, 63, 13, 71, 16, 58, 62, 18, 37, 74, 46, 13, 26, 60, 31, 83, 22, 20, 21, 38, 86, 105, 4, 34, 8, 18, 32, 35, 42, 17, 31, 35, 16, 29, 24, 30, 66, 50, 73, 20, 17, 10, 28, 14, 12, 40, 19, 58, 36, 25, 18, 24, 21, 35, 20, 50, 35, 35, 21, 32, 15, 57, 15, 28, 46, 8, 33, 30, 49, 47, 24, 74, 36, 23, 66, 16, 43, 26, 23, 33, 16, 19, 18, 62, 24, 29, 21, 13, 19, 52, 19, 22, 18, 40, 18, 53, 11, 34, 21, 61, 18, 66, 20, 66, 31, 64, 18, 32, 20, 41, 18, 57, 29, 24, 108, 17, 44, 28, 10, 22, 35, 16, 17, 20, 40, 28, 34, 19, 16, 10, 33, 51, 41, 34, 35, 31, 32, 39, 38, 22, 18, 67, 16, 34, 53, 17, 16, 17, 17, 65, 17, 89, 72, 55, 43, 49, 33, 8, 17, 29, 31, 60, 21, 63, 81, 26, 28, 75, 56, 18, 25, 9, 31, 77, 20, 25, 17, 33, 34, 23, 21, 25, 18, 41, 14, 32, 21, 12, 84, 27, 33, 43, 18, 18, 40, 30, 72, 64, 25, 47, 14, 64, 9, 24, 28, 15, 12, 17, 31, 55, 51, 57, 38, 17, 62, 7, 23, 28, 22, 10, 22, 30, 19, 27, 67, 37, 21, 37, 72, 40, 9, 72, 18, 7, 22, 33, 103, 27, 11, 20, 22, 21, 54, 41, 21, 23, 21, 20, 48, 24, 55, 11, 39, 72, 65, 36, 30, 25, 28, 135, 50, 66, 15, 27, 44, 26, 25, 10, 51, 41, 12, 67, 10, 61, 31, 46, 22, 15, 25, 31, 19, 26, 29, 38, 37, 24, 21, 28, 50, 28, 22, 36, 14, 39, 41, 67, 39, 12, 41, 8, 30, 14, 32, 25, 53, 4, 10, 32, 79, 17, 66, 26, 18, 68, 42, 23, 6, 35, 66, 32, 23, 28, 60, 17, 43, 50, 47, 90, 36, 35, 30, 23, 40, 42, 67, 34, 22, 27, 33, 50, 27, 57, 43, 32, 17, 50, 20, 35, 29, 23, 59, 85, 34, 31, 30, 87, 53, 27, 16, 22, 11, 68, 26, 18, 51, 34, 19, 22, 37, 29, 16, 11, 20, 89, 27, 17, 28, 29, 25, 34, 7, 36, 17, 26, 23, 16, 42, 30, 28, 37, 37, 13, 41, 29, 59, 65, 75, 73, 34, 38, 34, 19, 55, 17, 63, 12, 48, 16, 44, 19, 27, 21, 22, 34, 41, 36, 52, 11, 38, 29, 25, 20, 20, 26, 29, 35, 21, 24, 26, 22, 20, 26, 52, 19, 30, 31, 44, 18, 11, 21, 19, 11, 20, 46, 70, 19, 64, 18, 24, 21, 17, 52, 18, 42, 72, 32, 16, 45, 47, 24, 67, 25, 21, 44, 21, 14, 35, 61, 23, 43, 15, 20, 33, 39, 76, 35, 15, 25, 18, 39, 19, 43, 32, 32, 51, 33, 29, 78, 25, 28, 26, 20, 47, 17, 11, 56, 28, 18, 59, 11, 27, 13, 25, 22, 26, 57, 26, 52, 46, 13, 24, 24, 27, 22, 40, 45, 100, 34, 14, 55, 24, 45, 33, 10, 75, 33, 92, 27, 24, 33, 35, 17, 16, 28, 88, 60, 23, 23, 42, 55, 21, 35, 8, 61, 29, 57, 20, 23, 13, 4, 12, 34, 16, 14, 51, 17, 77, 17, 18, 24, 58, 25, 47, 14, 6, 21, 24, 37, 32, 22, 71, 30, 44, 30, 13, 33, 45, 46, 38, 98, 10, 12, 25, 24, 29, 42, 41, 55, 59, 57, 9, 35, 65, 47, 64, 18, 63, 49, 18, 86, 27, 40, 49, 38, 79, 24, 34, 17, 33, 15, 34, 25, 47, 34, 17, 8, 22, 33, 24, 38, 81, 33, 78, 42, 63, 73, 48, 18, 63, 47, 18, 52, 21, 37, 24, 41, 26, 56, 37, 34, 42, 72, 21, 53, 22, 20, 9, 53, 24, 23, 23, 37, 10, 45, 59, 65, 63, 17, 24, 27, 28, 14, 25, 30, 22, 68, 31, 11, 57, 48, 27, 21, 17, 32, 86, 63, 25, 28, 15, 12, 26, 55, 24, 19, 27, 34, 30, 62, 16, 60, 74, 33, 23, 27, 41, 17, 62, 29, 20, 26, 69, 29, 26, 33, 96, 15, 63, 68, 42, 50, 28, 17, 47, 31, 16, 18, 14, 38, 43, 18, 20, 16, 54, 18, 33, 26, 72, 29, 27, 70, 13, 89, 28, 89, 28, 83, 72, 29, 31, 23, 20, 14, 20, 14, 21, 18, 22, 85, 93, 37, 29, 19, 69, 34, 40, 18, 49, 13, 12, 40, 33, 19, 49, 44, 37, 57, 42, 68, 20, 45, 85, 25, 25, 60, 22, 34, 10, 77, 43, 23, 17, 19, 26, 20, 50, 57, 75, 28, 37, 22, 10, 20, 29, 24, 23, 60, 51, 41, 63, 41, 24, 92, 43, 62, 33, 22, 55, 44, 56, 39, 21, 74, 33, 33, 24, 27, 23, 38, 159, 24, 16, 16, 16, 31, 31, 71, 23, 57, 86, 19, 32, 51, 24, 77, 12, 61, 18, 24, 13, 56, 64, 75, 31, 25, 59, 34, 26, 49, 24, 14, 27, 35, 54, 19, 67, 24, 71, 25, 13, 38, 92, 60, 7, 12, 19, 37, 27, 18, 12, 91, 22, 85, 21, 73, 60, 16, 47, 25, 59, 21, 9, 10, 33, 18, 74, 56, 72, 72, 70, 15, 21, 24, 23, 23, 8, 31, 29, 72, 16, 33, 28, 37, 45, 30, 14, 23, 70, 23, 26, 123, 118, 49, 29, 27, 171, 39, 30, 46, 17, 74, 10, 31, 49, 61, 19, 76, 13, 76, 39, 23, 25, 65, 24, 37, 21, 21, 27, 18, 24, 19, 17, 114, 24, 34, 38, 42, 65, 69, 41, 71, 18, 16, 11, 26, 38, 86, 49, 24, 29, 25, 16, 27, 14, 48, 33, 71, 21, 24, 56, 63, 17, 6, 63, 20, 63, 25, 73, 26, 19, 39, 40, 28, 18, 18, 69, 17, 81, 48, 25, 28, 23, 27, 28, 54, 25, 36, 64, 7, 17, 43, 16, 33, 11, 45, 31, 75, 16, 23, 22, 19, 15, 28, 32, 20, 47, 32, 65, 19, 27, 25, 28, 42, 22, 13, 22, 50, 89, 24, 14, 46, 71, 29, 20, 14, 17, 47, 45, 97, 12, 45, 25, 21, 55, 12, 32, 27, 66, 34, 20, 99, 22, 43, 17, 54, 49, 26, 33, 78, 52, 27, 53, 62, 73, 54, 34, 63, 13, 22, 82, 49, 38, 26, 37, 38, 17, 24, 18, 16, 23, 28, 23, 14, 16, 24, 58, 59, 47, 21, 70, 87, 61, 76, 67, 34, 34, 26, 15, 20, 19, 51, 38, 21, 27, 16, 44, 18, 30, 31, 70, 23, 16, 21, 23, 29, 20, 33, 41, 19, 23, 32, 15, 21, 22, 20, 16, 24, 49, 21, 33, 38, 23, 14, 13, 28, 62, 44, 25, 20, 45, 19, 35, 83, 51, 23, 41, 66, 29, 40, 21, 27, 31, 30, 44, 24, 22, 32, 88, 10, 25, 36, 29, 26, 44, 71, 19, 34, 16, 20, 14, 46, 22, 17, 57, 19, 45, 15, 75, 72, 29, 79, 21, 23, 30, 23, 20, 129, 31, 15, 63, 22, 59, 51, 18, 58, 31, 19, 13, 37, 63, 18, 17, 4, 15, 34, 22, 65, 10, 67, 75, 85, 38, 24, 13, 28, 39, 22, 24, 50, 46, 52, 26, 64, 20, 44, 19, 55, 23, 49, 26, 79, 22, 75, 32, 73, 20, 56, 24, 42, 34, 54, 22, 13, 14, 17, 26, 16, 12, 22, 36, 18, 29, 44, 34, 38, 17, 79, 29, 21, 40, 32, 55, 99, 57, 35, 15, 23, 96, 13, 63, 29, 33, 13, 50, 26, 17, 61, 19, 30, 20, 17, 25, 46, 31, 17, 17, 38, 34, 37, 44, 23, 31, 12, 78, 28, 17, 4, 30, 34, 27, 38, 19, 48, 57, 30, 37, 57, 31, 25, 16, 72, 20, 67, 35, 29, 12, 14, 20, 34, 11, 58, 54, 47, 41, 87, 25, 15, 32, 50, 182, 4, 21, 26, 32, 50, 23, 30, 32, 24, 32, 16, 25, 49, 64, 35, 23, 9, 35, 35, 18, 71, 29, 29, 25, 30, 20, 22, 22, 24, 39, 22, 48, 33, 31, 67, 67, 12, 12, 10, 18, 23, 54, 21, 70, 23, 17, 35, 39, 8, 66, 64, 26, 31, 31, 23, 11, 34, 19, 27, 24, 43, 75, 31, 38, 47, 16, 49, 27, 31, 23, 33, 17, 46, 16, 46, 32, 108, 47, 19, 26, 43, 64, 66, 24, 65, 44, 37, 32, 16, 24, 83, 18, 43, 15, 22, 19, 49, 16, 34, 17, 28, 22, 40, 41, 20, 43, 60, 72, 20, 17, 43, 10, 56, 35, 22, 36, 32, 57, 37, 30, 16, 27, 43, 79, 51, 47, 41, 65, 22, 20, 13, 24, 30, 53, 64, 21, 25, 24, 58, 31, 17, 75, 11, 26, 67, 24, 26, 33, 9, 78, 143, 54, 25, 78, 44, 25, 34, 30, 37, 16, 22, 30, 17, 23, 24, 37, 18, 62, 58, 19, 119, 56, 75, 65, 12, 45, 41, 17, 14, 25, 30, 47, 22, 35, 19, 15, 61, 32, 49, 23, 18, 41, 10, 36, 57, 61, 20, 41, 58, 17, 32, 73, 15, 49, 59, 33, 71, 36, 28, 18, 196, 18, 66, 81, 12, 75, 15, 26, 16, 17, 28, 43, 19, 31, 15, 40, 23, 9, 11, 62, 23, 25, 36, 7, 79, 49, 53, 31, 43, 17, 17, 68, 68, 35, 46, 75, 32, 27, 36, 21, 68, 15, 45, 51, 16, 17, 31, 43, 18, 39, 25, 62, 33, 52, 48, 42, 16, 67, 16, 12, 36, 47, 32, 14, 33, 53, 41, 23, 70, 25, 51, 20, 18, 39, 46, 98, 34, 12, 44, 11, 19, 27, 22, 46, 15, 19, 44, 74, 12, 38, 25, 94, 30, 16, 84, 24, 57, 55, 21, 28, 88, 72, 27, 11, 43, 43, 72, 49, 26, 28, 22, 49, 16, 28, 44, 24, 45, 35, 57, 91, 36, 142, 27, 15, 62, 24, 10, 26, 64, 20, 39, 21, 35, 59, 17, 19, 26, 15, 61, 13, 50, 53, 44, 20, 55, 15, 19, 30, 27, 19, 34, 22, 33, 23, 72, 18, 17, 24, 57, 34, 34, 9, 30, 7, 25, 25, 11, 24, 58, 87, 31, 64, 26, 36, 21, 24, 41, 18, 28, 87, 40, 14, 34, 34, 30, 21, 29, 32, 58, 34, 40, 19, 26, 27, 20, 10, 56, 21, 64, 22, 24, 65, 22, 32, 40, 43, 26, 23, 33, 38, 79, 17, 67, 33, 9, 20, 15, 61, 32, 34, 68, 34, 97, 26, 72, 39, 36, 12, 47, 51, 28, 9, 57, 8, 20, 16, 29, 70, 44, 58, 11, 17, 46, 61, 31, 21, 43, 28, 39, 25, 32, 81, 46, 24, 35, 35, 20, 29, 15, 44, 60, 22, 16, 14, 42, 48, 12, 32, 15, 27, 27, 20, 60, 34, 19, 14, 33, 22, 20, 15, 22, 27, 43, 49, 27, 43, 30, 55, 50, 33, 18, 21, 24, 62, 27, 30, 17, 42, 33, 74, 29, 25, 46, 74, 36, 27, 71, 14, 23, 23, 43, 23, 21, 58, 35, 64, 17, 51, 26, 64, 54, 53, 11, 16, 11, 17, 11, 19, 19, 18, 33, 19, 35, 82, 73, 72, 28, 33, 32, 89, 23, 28, 4, 76, 34, 14, 24, 46, 33, 11, 52, 24, 30, 31, 53, 25, 35, 87, 30, 69, 27, 56, 26, 13, 49, 63, 25, 25, 22, 71, 61, 15, 7, 32, 10, 20, 12, 44, 38, 16, 72, 21, 28, 24, 33, 9, 28, 18, 60, 71, 68, 54, 20, 29, 17, 21, 17, 32, 45, 79, 42, 17, 26, 60, 6, 18, 18, 20, 48, 11, 29, 10, 34, 30, 18, 18, 29, 18, 19, 56, 85, 137, 69, 20, 36, 42, 20, 11, 47, 103, 32, 17, 55, 19, 26, 28, 9, 43, 25, 20, 38, 10, 41, 53, 23, 70, 48, 19, 20, 16, 35, 35, 31, 13, 162, 67, 26, 28, 15, 46, 21, 61, 45, 11, 36, 28, 33, 26, 42, 37, 57, 29, 17, 58, 72, 32, 18, 17, 86, 13, 24, 33, 19, 29, 34, 35, 20, 70, 128, 88, 44, 34, 16, 45, 62, 20, 107, 21, 27, 63, 30, 41, 54, 29, 29, 25, 20, 27, 37, 34, 77, 52, 17, 70, 24, 18, 23, 15, 7, 35, 45, 35, 28, 26, 31, 23, 32, 24, 13, 20, 50, 62, 27, 64, 36, 66, 51, 24, 4, 37, 24, 26, 15, 21, 21, 26, 29, 58, 27, 35, 18, 53, 23, 18, 20, 31, 62, 15, 29, 52, 22, 7, 18, 30, 85, 23, 27, 27, 18, 28, 56, 39, 19, 24, 29, 208, 126, 11, 88, 22, 40, 11, 29, 44, 35, 25, 63, 25, 27, 34, 20, 34, 17, 18, 12, 64, 52, 61, 21, 16, 17, 89, 58, 17, 34, 32, 32, 46, 45, 41, 28, 43, 57, 25, 53, 22, 22, 67, 15, 44, 13, 25, 34, 165, 31, 12, 9, 59, 46, 34, 30, 67, 7, 38, 61, 32, 58, 55, 26, 14, 12, 43, 16, 30, 16, 49, 62, 32, 23, 25, 25, 16, 25, 30, 37, 26, 38, 17, 46, 75, 55, 28, 21, 35, 55, 48, 62, 29, 58, 42, 8, 32, 33, 17, 12, 51, 34, 42, 33, 56, 40, 17, 32, 25, 34, 43, 23, 13, 22, 66, 4, 60, 23, 25, 33, 10, 67, 28, 20, 32, 22, 35, 37, 75, 13, 47, 16, 31, 29, 41, 30, 48, 22, 30, 23, 44, 28, 26, 33, 24, 37, 57, 51, 77, 55, 38, 33, 35, 12, 73, 18, 81, 26, 45, 38, 76, 19, 47, 33, 11, 11, 30, 41, 26, 26, 43, 28, 22, 31, 74, 33, 37, 24, 23, 40, 20, 23, 21, 25, 13, 22, 56, 35, 29, 85, 43, 16, 39, 24, 72, 26, 23, 57, 14, 23, 20, 15, 17, 45, 24, 42, 16, 35, 13, 16, 28, 33, 20, 31, 54, 21, 22, 76, 51, 12, 31, 23, 26, 17, 47, 48, 29, 8, 27, 25, 48, 16, 38, 100, 20, 15, 47, 29, 10, 29, 74, 23, 22, 39, 30, 45, 54, 18, 51, 18, 22, 35, 29, 18, 14, 36, 27, 27, 26, 51, 32, 12, 79, 25, 101, 58, 22, 58, 44, 16, 33, 15, 9, 23, 57, 33, 21, 12, 13, 36, 33, 4, 62, 28, 57, 23, 88, 17, 41, 41, 31, 58, 24, 84, 36, 32, 36, 47, 8, 18, 36, 33, 25, 51, 15, 21, 57, 29, 55, 70, 20, 30, 34, 67, 54, 33, 54, 16, 66, 62, 26, 15, 52, 64, 29, 38, 37, 53, 56, 28, 66, 29, 40, 15, 32, 84, 40, 62, 23, 34, 27, 16, 33, 21, 34, 35, 108, 71, 18, 64, 22, 42, 58, 18, 31, 11, 30, 16, 16, 56, 49, 20, 25, 32, 19, 43, 23, 24, 12, 40, 40, 55, 49, 52, 25, 29, 37, 29, 17, 30, 16, 16, 15, 15, 41, 34, 17, 73, 56, 49, 50, 94, 36, 29, 89, 28, 55, 22, 36, 72, 13, 33, 25, 8, 79, 25, 83, 57, 36, 19, 25, 80, 42, 53, 18, 65, 17, 15, 16, 30, 22, 17, 24, 4, 17, 19, 60, 7, 156, 13, 31, 21, 14, 63, 42, 65, 41, 30, 25, 15, 72, 14, 18, 27, 25, 25, 26, 32, 66, 38, 66, 26, 17, 15, 15, 21, 32, 67, 66, 24, 20, 19, 27, 52, 41, 18, 18, 51, 23, 23, 15, 93, 40, 33, 53, 33, 53, 68, 11, 70, 26, 40, 65, 41, 64, 62, 29, 33, 15, 13, 9, 62, 19, 73, 17, 35, 25, 57, 32, 32, 23, 70, 63, 193, 25, 21, 64, 32, 46, 20, 20, 47, 67, 75, 46, 94, 33, 72, 46, 77, 21, 32, 61, 24, 48, 87, 33, 9, 35, 44, 19, 19, 14, 72, 33, 27, 37, 44, 24, 9, 19, 25, 30, 9, 17, 17, 31, 29, 16, 34, 29, 52, 9, 38, 24, 31, 48, 13, 55, 31, 17, 48, 37, 14, 67, 51, 52, 30, 19, 30, 27, 42, 19, 12, 34, 36, 19, 17, 59, 19, 25, 6, 26, 71, 26, 47, 30, 19, 11, 41, 16, 70, 37, 65, 21, 4, 67, 31, 48, 22, 26, 29, 16, 35, 22, 29, 34, 35, 23, 27, 75, 45, 26, 32, 88, 18, 10, 30, 39, 21, 35, 16, 32, 47, 18, 50, 39, 31, 27, 30, 69, 35, 34, 30, 35, 67, 43, 20, 26, 32, 36, 37, 13, 29, 39, 28, 18, 61, 72, 68, 34, 20, 25, 31, 65, 33, 29, 35, 28, 19, 26, 22, 20, 43, 15, 68, 21, 14, 77, 38, 17, 81, 36, 64, 21, 29, 66, 66, 16, 21, 7, 20, 71, 33, 36, 35, 40, 50, 19, 15, 49, 20, 27, 32, 41, 75, 34, 12, 81, 12, 16, 51, 42, 29, 17, 47, 13, 15, 55, 31, 13, 40, 34, 56, 11, 84, 43, 28, 21, 71, 61, 12, 4, 17, 16, 61, 18, 34, 67, 15, 8, 48, 15, 38, 68, 51, 22, 38, 30, 15, 72, 17, 63, 39, 20, 13, 23, 37, 18, 17, 32, 53, 14, 28, 27, 50, 29, 31, 15, 42, 67, 37, 19, 22, 44, 47, 68, 17, 25, 34, 22, 36, 57, 12, 36, 60, 37, 32, 22, 71, 63, 19, 36, 19, 22, 35, 20, 36, 24, 75, 41, 74, 52, 17, 23, 76, 15, 27, 67, 63, 36, 17, 30, 27, 59, 23, 16, 24, 44, 24, 37, 25, 16, 67, 54, 38, 170, 22, 38, 19, 19, 18, 32, 80, 30, 24, 25, 32, 28, 31, 39, 65, 21, 103, 16, 173, 28, 33, 20, 32, 37, 23, 28, 37, 28, 29, 56, 16, 16, 27, 16, 33, 40, 22, 91, 36, 38, 29, 24, 19, 30, 65, 71, 46, 68, 27, 60, 73, 24, 42, 13, 11, 27, 63, 4, 34, 75, 46, 16, 44, 52, 24, 34, 49, 8, 16, 34, 42, 24, 36, 83, 40, 25, 10, 15, 10, 36, 24, 31, 52, 19, 12, 17, 18, 62, 29, 24, 29, 31, 23, 25, 23, 21, 22, 33, 63, 22, 41, 43, 75, 21, 15, 38, 17, 43, 66, 12, 41, 40, 57, 18, 51, 13, 30, 79, 44, 20, 38, 44, 79, 45, 77, 80, 34, 30, 48, 45, 53, 26, 28, 30, 24, 53, 58, 121, 21, 52, 33, 52, 29, 20, 53, 67, 26, 52, 26, 32, 15, 59, 23, 17, 23, 24, 36, 16, 25, 25, 43, 42, 20, 37, 42, 25, 21, 14, 57, 25, 22, 12, 21, 65, 43, 16, 21, 68, 42, 17, 75, 47, 12, 38, 22, 19, 37, 72, 19, 55, 9, 47, 23, 9, 34, 16, 24, 19, 27, 33, 46, 21, 29, 19, 18, 70, 21, 21, 36, 16, 61, 65, 11, 20, 21, 34, 63, 81, 13, 13, 108, 32, 19, 18, 20, 36, 40, 48, 22, 51, 65, 75, 35, 13, 48, 23, 45, 30, 39, 23, 14, 14, 49, 71, 8, 58, 46, 27, 26, 60, 44, 19, 23, 28, 12, 53, 18, 16, 18, 38, 20, 59, 29, 65, 52, 16, 34, 14, 26, 17, 27, 17, 33, 17, 17, 67, 26, 19, 68, 54, 30, 71, 12, 18, 37, 27, 33, 53, 16, 15, 20, 26, 75, 60, 27, 34, 74, 30, 97, 18, 54, 63, 22, 24, 33, 52, 17, 20, 63, 45, 14, 34, 20, 68, 32, 30, 50, 64, 26, 41, 32, 25, 24, 31, 23, 38, 12, 13, 51, 63, 24, 76, 27, 35, 87, 36, 63, 16, 102, 25, 27, 16, 27, 31, 34, 32, 49, 31, 18, 46, 29, 20, 56, 34, 23, 64, 20, 63, 39, 11, 65, 50, 27, 43, 34, 46, 45, 10, 56, 61, 37, 22, 15, 19, 21, 4, 52, 58, 31, 73, 19, 48, 30, 32, 29, 41, 18, 17, 9, 18, 14, 14, 6, 47, 29, 24, 16, 88, 15, 50, 40, 12, 25, 40, 45, 72, 15, 56, 71, 15, 30, 13, 27, 34, 12, 8, 43, 28, 36, 38, 24, 43, 39, 54, 62, 54, 18, 15, 29, 41, 6, 21, 27, 42, 48, 11, 14, 38, 34, 17, 22, 26, 27, 20, 19, 15, 28, 18, 21, 25, 20, 65, 11, 41, 88, 11, 31, 20, 34, 20, 28, 23, 58, 36, 71, 13, 39, 56, 33, 54, 72, 22, 73, 20, 59, 37, 37, 72, 28, 29, 24, 16, 32, 36, 20, 36, 38, 19, 23, 33, 35, 44, 38, 17, 24, 39, 28, 31, 58, 76, 29, 45, 31, 20, 31, 67, 27, 72, 28, 13, 73, 59, 41, 88, 33, 11, 64, 23, 32, 88, 44, 17, 50, 33, 56, 16, 17, 23, 27, 46, 15, 44, 24, 30, 20, 71, 45, 33, 32, 24, 33, 27, 39, 49, 27, 80, 56, 45, 78, 72, 33, 33, 34, 9, 42, 103, 41, 30, 13, 76, 27, 33, 33, 29, 51, 25, 16, 32, 9, 37, 54, 26, 22, 59, 31, 56, 59, 60, 27, 22, 21, 17, 30, 12, 58, 23, 16, 32, 31, 5, 21, 52, 13, 9, 22, 11, 61, 14, 4, 20, 39, 74, 29, 30, 60, 6, 32, 13, 78, 31, 39, 17, 30, 46, 23, 26, 20, 21, 71, 27, 72, 27, 17, 25, 33, 26, 54, 21, 76, 48, 55, 13, 72, 46, 50, 62, 11, 30, 28, 13, 26, 23, 63, 25, 9, 21, 50, 15, 56, 39, 24, 38, 9, 39, 17, 56, 29, 63, 62, 10, 14, 18, 22, 24, 46, 64, 21, 30, 87, 32, 44, 35, 32, 64, 28, 21, 59, 9, 33, 13, 22, 12, 57, 50, 39, 24, 31, 43, 44, 20, 9, 41, 62, 57, 17, 68, 33, 30, 31, 35, 31, 30, 32, 19, 29, 25, 65, 45, 23, 36, 24, 45, 38, 54, 59, 28, 13, 51, 41, 62, 47, 19, 78, 29, 28, 57, 15, 40, 36, 11, 42, 20, 10, 29, 17, 39, 76, 40, 6, 39, 15, 85, 30, 25, 28, 72, 69, 25, 20, 37, 46, 32, 22, 67, 47, 31, 19, 26, 57, 31, 33, 45, 35, 55, 29, 17, 12, 36, 57, 23, 18, 23, 44, 72, 27, 26, 30, 20, 41, 18, 31, 15, 28, 58, 25, 47, 22, 64, 54, 85, 35, 67, 24, 23, 30, 87, 50, 13, 63, 13, 77, 119, 16, 17, 16, 19, 61, 43, 21, 37, 41, 32, 27, 23, 30, 76, 18, 10, 43, 13, 24, 18, 25, 67, 18, 18, 20, 43, 27, 41, 17, 26, 59, 32, 26, 40, 65, 36, 28, 52, 48, 67, 64, 34, 4, 63, 39, 12, 64, 26, 30, 15, 18, 42, 17, 74, 31, 49, 30, 10, 35, 25, 20, 37, 64, 19, 66, 59, 75, 37, 25, 72, 25, 35, 108, 19, 125, 55, 55, 28, 25, 59, 63, 17, 31, 16, 32, 62, 87, 47, 68, 63, 35, 48, 57, 11, 25, 17, 60, 17, 21, 23, 35, 37, 62, 24, 65, 22, 37, 24, 69, 58, 56, 25, 22, 31, 67, 15, 72, 18, 17, 17, 23, 27, 75, 58, 30, 29, 60, 15, 20, 15, 22, 31, 8, 42, 34, 14, 21, 67, 28, 33, 21, 36, 12, 40, 21, 58, 35, 95, 30, 57, 14, 24, 39, 23, 20, 49, 30, 11, 57, 20, 17, 71, 30, 20, 33, 31, 24, 75, 20, 28, 18, 19, 61, 39, 61, 56, 55, 45, 18, 35, 36, 60, 9, 17, 28, 48, 38, 24, 44, 8, 55, 77, 11, 52, 21, 31, 70, 36, 28, 15, 25, 68, 26, 27, 42, 46, 37, 26, 43, 26, 46, 14, 34, 12, 15, 17, 18, 28, 36, 70, 10, 79, 17, 31, 37, 30, 22, 54, 44, 32, 50, 50, 34, 33, 17, 13, 32, 17, 22, 31, 40, 12, 97, 40, 70, 85, 29, 20, 78, 33, 83, 27, 23, 59, 21, 75, 27, 17, 45, 43, 19, 33, 22, 47, 40, 44, 8, 16, 73, 65, 17, 36, 30, 12, 16, 46, 18, 24, 29, 51, 33, 13, 71, 26, 37, 30, 54, 56, 67, 20, 32, 28, 84, 121, 9, 27, 38, 33, 52, 17, 11, 7, 29, 37, 43, 44, 24, 17, 11, 31, 94, 34, 31, 26, 21, 14, 56, 29, 46, 59, 14, 20, 53, 49, 33, 23, 22, 25, 11, 29, 24, 48, 4, 21, 34, 20, 46, 71, 88, 19, 21, 46, 27, 16, 19, 46, 18, 24, 35, 80, 24, 18, 60, 19, 28, 17, 17, 16, 63, 33, 31, 17, 48, 55, 23, 87, 25, 30, 13, 35, 40, 42, 45, 29, 18, 12, 30, 36, 49, 26, 54, 13, 68, 37, 48, 12, 15, 32, 26, 58, 64, 67, 27, 35, 15, 61, 15, 64, 23, 41, 24, 14, 20, 39, 12, 24, 69, 17, 12, 19, 19, 16, 11, 33, 29, 16, 28, 40, 38, 47, 40, 15, 31, 18, 25, 23, 35, 29, 25, 25, 16, 15, 40, 20, 69, 61, 15, 14, 24, 72, 70, 15, 43, 73, 62, 219, 38, 10, 47, 30, 16, 30, 29, 50, 25, 59, 8, 23, 15, 66, 25, 26, 25, 23, 23, 24, 33, 29, 13, 28, 71, 46, 30, 65, 25, 28, 36, 49, 31, 68, 49, 49, 24, 31, 25, 20, 66, 30, 18, 25, 79, 71, 58, 36, 67, 36, 19, 4, 31, 15, 40, 17, 30, 65, 51, 28, 5, 29, 9, 14, 36, 47, 54, 31, 4, 17, 26, 18, 60, 72, 30, 27, 52, 58, 11, 16, 43, 43, 18, 14, 17, 31, 28, 31, 20, 25, 30, 13, 18, 71, 80, 45, 18, 23, 20, 13, 9, 26, 93, 23, 21, 86, 60, 23, 41, 27, 22, 32, 53, 17, 27, 45, 32, 72, 20, 36, 19, 43, 53, 27, 30, 54, 20, 63, 50, 28, 13, 21, 17, 54, 52, 45, 19, 31, 60, 37, 16, 37, 29, 67, 66, 33, 31, 19, 41, 37, 11, 24, 21, 29, 25, 83, 57, 108, 40, 41, 22, 26, 46, 63, 33, 15, 31, 27, 73, 36, 30, 21, 69, 23, 68, 33, 34, 16, 28, 59, 23, 31, 19, 23, 18, 44, 27, 19, 21, 58, 73, 17, 28, 50, 14, 57, 32, 37, 23, 26, 27, 28, 49, 45, 23, 44, 36, 31, 21, 28, 29, 15, 49, 32, 68, 37, 88, 62, 19, 42, 42, 33, 23, 15, 44, 17, 31, 22, 46, 92, 31, 56, 63, 36, 15, 78, 32, 9, 44, 11, 39, 19, 27, 42, 57, 19, 25, 32, 36, 56, 45, 66, 22, 15, 31, 28, 30, 14, 36, 29, 21, 22, 13, 39, 126, 28, 67, 13, 31, 61, 26, 59, 37, 70, 8, 11, 59, 21, 42, 21, 22, 11, 67, 21, 28, 71, 18, 23, 34, 21, 24, 51, 69, 30, 41, 28, 4, 81, 8, 42, 83, 21, 19, 41, 40, 29, 13, 21, 16, 63, 32, 64, 12, 67, 67, 31, 64, 17, 42, 28, 36, 12, 32, 60, 39, 17, 45, 18, 19, 72, 47, 21, 89, 17, 35, 41, 39, 68, 34, 38, 14, 36, 45, 23, 68, 24, 10, 33, 72, 51, 30, 26, 16, 31, 57, 87, 17, 67, 35, 51, 55, 15, 81, 77, 12, 41, 45, 46, 74, 71, 30, 20, 72, 35, 24, 28, 22, 41, 4, 16, 19, 67, 47, 8, 18, 23, 41, 31, 29, 30, 38, 9, 48, 28, 14, 31, 31, 55, 23, 41, 28, 30, 54, 57, 10, 57, 4, 81, 17, 163, 57, 17, 25, 41, 26, 28, 43, 39, 13, 36, 14, 22, 61, 31, 37, 25, 18, 72, 98, 29, 33, 56, 27, 48, 17, 26, 63, 37, 10, 83, 17, 35, 23, 35, 25, 33, 26, 64, 40, 29, 24, 23, 12, 35, 37, 28, 35, 80, 42, 19, 30, 31, 28, 64, 53, 24, 34, 20, 28, 21, 36, 57, 43, 32, 26, 10, 60, 43, 25, 40, 23, 29, 39, 12, 13, 58, 15, 55, 67, 34, 27, 33, 23, 38, 74, 62, 25, 71, 29, 57, 24, 52, 23, 18, 91, 28, 13, 34, 68, 23, 16, 30, 35, 25, 23, 42, 39, 58, 32, 54, 14, 28, 75, 23, 37, 33, 40, 29, 18, 18, 20, 38, 28, 23, 20, 58, 11, 75, 160, 12, 34, 47, 32, 42, 12, 54, 67, 39, 46, 11, 17, 52, 31, 31, 24, 39, 65, 19, 39, 17, 36, 84, 16, 33, 16, 28, 32, 72, 10, 33, 25, 43, 50, 68, 65, 15, 46, 60, 19, 50, 27, 28, 24, 68, 30, 59, 66, 16, 23, 54, 32, 52, 37, 26, 70, 21, 35, 11, 38, 29, 16, 55, 37, 28, 28, 39, 41, 33, 17, 50, 20, 43, 81, 45, 33, 30, 56, 14, 22, 45, 30, 20, 32, 20, 38, 30, 25, 69, 23, 25, 24, 103, 34, 23, 23, 47, 29, 32, 10, 32, 27, 11, 38, 42, 36, 31, 13, 16, 34, 20, 16, 21, 34, 44, 14, 71, 23, 17, 15, 66, 69, 59, 21, 86, 61, 47, 36, 32, 20, 50, 38, 48, 36, 33, 39, 29, 14, 20, 19, 85, 40, 40, 22, 15, 23, 10, 24, 18, 13, 19, 76, 19, 75, 15, 30, 60, 34, 27, 47, 16, 82, 32, 46, 21, 38, 52, 14, 22, 22, 14, 48, 10, 21, 34, 19, 14, 21, 18, 22, 67, 27, 31, 14, 29, 43, 39, 40, 72, 23, 47, 16, 16, 22, 46, 34, 55, 37, 72, 17, 19, 25, 58, 17, 49, 12, 13, 14, 22, 17, 25, 57, 175, 99, 63, 52, 12, 12, 17, 38, 17, 66, 21, 37, 33, 13, 29, 62, 59, 27, 27, 48, 44, 36, 19, 15, 74, 43, 63, 52, 11, 21, 12, 60, 17, 26, 68, 28, 43, 21, 10, 30, 18, 35, 22, 30, 16, 17, 33, 19, 34, 67, 10, 26, 65, 31, 19, 34, 12, 4, 20, 28, 33, 528, 11, 66, 22, 21, 21, 60, 65, 42, 30, 27, 60, 59, 23, 26, 51, 11, 28, 44, 41, 17, 31, 29, 31, 36, 37, 36, 37, 28, 18, 78, 21, 12, 74, 38, 30, 65, 49, 17, 78, 40, 21, 27, 20, 45, 68, 20, 58, 34, 30, 36, 49, 29, 23, 4, 15, 18, 32, 14, 20, 55, 23, 33, 69, 20, 39, 54, 42, 28, 22, 76, 67, 23, 17, 31, 25, 21, 26, 32, 4, 35, 17, 21, 14, 18, 27, 29, 13, 20, 34, 34, 24, 10, 62, 29, 57, 26, 21, 21, 40, 9, 15, 11, 17, 30, 40, 16, 17, 7, 8, 21, 48, 48, 154, 20, 17, 10, 26, 41, 81, 12, 73, 72, 19, 56, 20, 87, 39, 23, 19, 19, 37, 29, 87, 22, 26, 61, 15, 20, 26, 28, 12, 25, 42, 44, 65, 28, 28, 25, 35, 55, 14, 31, 9, 19, 19, 24, 18, 20, 49, 34, 73, 39, 41, 36, 11, 26, 53, 52, 20, 34, 84, 36, 15, 38, 21, 17, 53, 32, 16, 24, 65, 32, 23, 58, 49, 40, 35, 12, 83, 33, 24, 35, 49, 67, 21, 57, 30, 98, 24, 22, 15, 27, 18, 23, 17, 61, 35, 9, 27, 30, 23, 16, 46, 23, 21, 26, 30, 11, 25, 19, 35, 30, 18, 21, 26, 34, 18, 38, 35, 68, 27, 17]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uAVTDwOE1mzH"},"source":["The code above shows that the maximum sentence and the average sentence length, on which the max sentence length can be based"]},{"cell_type":"markdown","metadata":{"id":"RKrfmS9z1vIc"},"source":["The `tokenizer.encode_plus` will do the acutal tokenization:\n","\n","\n","1.   Split the sentences into tokens\n","2.   Add the special `[CLS]` and `[SEP]` tokens\n","3.   Map the tokens to their IDS\n","4.   Pad or truncate all sentences to the same length\n","5.   Create the attention masks which explicitly differentiate real tokens from `[PAD]` tokens\n","\n","\n","NOTE: `tokenizer.encode` does not add the `[PAD]` tokens, therefore I will be using `tokenizer.encode_plus`\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yP14w7Xd2tcL","executionInfo":{"status":"ok","timestamp":1611777029058,"user_tz":-60,"elapsed":18921,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}},"outputId":"4a4d5b1d-7c31-4082-fceb-30905ff367d5"},"source":["# Tokenize all of the sentences and map the tokens to their word ID, both for training, as for test\n","\n","def prepare_bert_data(sentences):\n","  input_ids = []\n","  attention_masks = []\n","\n","  for sent in sentences:\n","    encoded_dict = tokenizer.encode_plus(sent, # the sentence\n","                                        add_special_tokens = True,  # add [CLS] and [SEP] tokens\n","                                        max_length = 100,   # PAD and truncate all sentences\n","                                        pad_to_max_length = True,\n","                                        truncation=True,  \n","                                        return_attention_mask = True, # Construct attn. masks\n","                                        return_tensors = 'pt'       # Return pytorch tensors\n","                                        )\n","\n","    # Add the encoded sentence to the list\n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    # Add its attention mask (Differentiates padding from non-padding)\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","  # Converting the lists into tensors\n","  input_ids = torch.cat(input_ids, dim = 0)\n","  attention_masks = torch.cat(attention_masks, dim = 0)\n","\n","  return input_ids, attention_masks\n","\n","train_input_ids, train_attention_masks = prepare_bert_data(train_clean)\n","train_labels = torch.tensor(train_labels)\n","\n","dev_input_ids, dev_attention_masks = prepare_bert_data(dev_clean)\n","dev_labels = torch.tensor(dev_labels)\n","\n","test_input_ids, test_attention_masks = prepare_bert_data(test_clean)\n","test_labels = torch.tensor(test_labels)\n","\n","# Print sentence 0, now a list of IDS.\n","print('Original Dev: ', dev_clean[0])\n","print('Token IDs: ', dev_input_ids[0])"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Original Dev:  @USER @USER In #Breda worden kinderen gefilmd door #huisarts Baban&amp;Kappelhof. @USER klokkenluider nu dood. Lees tweets @USER \n","#pedos #depla\n","Token IDs:  tensor([    1,     0,  7127, 23317,     0,  7127, 23317,  3570,     0,  1440,\n","        22591, 14261, 12018, 10871,     0, 13480,  1020, 23967,     7,     0,\n","           26,  3898, 27726, 25957,    13,     0,  7127, 23317, 14369, 16155,\n","        10850,    13,  4276, 20433,   132,   131,     0,  7127, 23317,     0,\n","        17343, 24921,     0, 10537, 27792,     2,     3,     3,     3,     3,\n","            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","            3,     3,     3,     3,     3,     3,     3,     3,     3,     3])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_qBaqBaE4k9R"},"source":["# Training & Validating Split\n","For the thesis task, we have already divided the data into the final split"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YvIXStGZ4wrm","executionInfo":{"status":"ok","timestamp":1611777029060,"user_tz":-60,"elapsed":18869,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}},"outputId":"7511ff3d-73c6-47cd-8952-f1cd78ae3532"},"source":["# Combining the training inputs into a TensorDataset\n","train_ = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n","dev_ = TensorDataset(dev_input_ids, dev_attention_masks, dev_labels)\n","test_ = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n","\n","print('{:>5,} training samples'.format(len(train_)))\n","print('{:>5,} validation samples'.format(len(dev_)))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["5,706 training samples\n","  549 validation samples\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UUnbvKVa6Tba"},"source":["Below, I will create an iterator for my dataset, by using the torch `DataLoader` class. This helps save on memory during training beause, unlinke a for loop, with iterator the entire dataset does not need to be loaded into memory."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xb6hIYWL63M5","executionInfo":{"status":"ok","timestamp":1611777029061,"user_tz":-60,"elapsed":18836,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}},"outputId":"ab86bf1b-3f2d-4bde-a512-c564348fd241"},"source":["print('Initializing Dataloaders......')\n","# Specifying the batch size, for fine tuning, 16 or 32 is recommended.\n","batch_size = 32\n","\n","# Creating the dataloaders, train is randomlly sampled, test is sequentially sampled\n","train_dataloader = DataLoader(train_,\n","                              sampler = RandomSampler(train),\n","                              batch_size = batch_size\n","                              )\n","\n","validation_dataloader = DataLoader(dev_,\n","                                    sampler = RandomSampler(dev), # Original was a SequentialSampler(test)\n","                                    batch_size = batch_size\n","                                  )\n","\n","test_dataloader = DataLoader(test_,\n","                            sampler = SequentialSampler(test),\n","                            batch_size = batch_size\n","                            )"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Initializing Dataloaders......\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"taoTZ7lW8dLm"},"source":["# Training the Classification Model\n","Now that I have tokenized and loaded all the data, I will fine-tune BERT for sequence classification"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eLfsmVxU8nh3","executionInfo":{"status":"ok","timestamp":1611777036172,"user_tz":-60,"elapsed":25904,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}},"outputId":"5e6ee887-6261-4fa1-b3fc-661c12c46949"},"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","\n","# Loading M-bert base-uncased 102 languages - binary\n","#model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased', \n","#                                                        num_labels = 2,       # The number of output labels for binary classifcation\n","#                                                        output_attentions = False, # Whether or not the model returns attentions weights\n","#                                                        output_hidden_states = False  # Whether or not the model returns all hidden states\n","#                                                      )\n","\n","\n","# Loading BERTje model - binary:\n","#model = BertForSequenceClassification.from_pretrained('GroNLP/bert-base-dutch-cased', \n","#                                                        num_labels = 2,       # The number of output labels for binary classifcation\n","#                                                        output_attentions = False, # Whether or not the model returns attentions weights\n","#                                                        output_hidden_states = False  # Whether or not the model returns all hidden states\n","#                                                      )\n","\n","# Loading BERTje model - ternary:\n","model = BertForSequenceClassification.from_pretrained('GroNLP/bert-base-dutch-cased', \n","                                                        num_labels = 3,       # The number of output labels for binary classifcation\n","                                                        output_attentions = False, # Whether or not the model returns attentions weights\n","                                                        output_hidden_states = False  # Whether or not the model returns all hidden states\n","                                                      )\n","\n","\n","# Telling pytorch to run this model on the GPU\n","model.cuda()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at GroNLP/bert-base-dutch-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30000, 768, padding_idx=3)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"QA2nNsZO9-ML"},"source":["Inspecting the BERT model"]},{"cell_type":"code","metadata":{"id":"ArNjhAfkACxG","executionInfo":{"status":"ok","timestamp":1611777036176,"user_tz":-60,"elapsed":25886,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}}},"source":["from transformers import get_linear_schedule_with_warmup\n","# Initializing the optimizer, learning rate = 2e-5 and the epsilon is set to 1e-8 to prevent any zero division errors\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5,\n","                  eps = 1e-8)\n","\n","# I have chosen 4 epochs at the moment, but this might cause overfitting\n","epochs = 5\n","\n","# Calculating how many steps have to be taken\n","total_steps = len(train_dataloader) * epochs\n","\n","# Creating the learning rate scheduler\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Urx7yUvyAjfE"},"source":["**Creating the training loop for the classification task**\n","\n","The process for this loop is taken from the example which states:\n","\n","**Training:**\n","\n","\n","*   Unpack our data inputs and labels\n","*   Load data onto GPU for acceleration\n","*   Clear out the gradients calculated in the previous pass\n","*   Forward pass\n","*   Backward pass\n","*   Tell the network to update parameters with `optimizer.step()`\n","*   Track variables for monitoring progress\n","\n","\n","**Evaluation**\n","\n","\n","*   Unpack our data inputs and labels\n","*   Load data onto the GPU for acceleration\n","*   Forward pass (feed input data through network)\n","*   Comput loss on our validation data and track variables for monitoring progress\n","\n","\n","Below, I have first created a helper function that can calculate the Accuracy."]},{"cell_type":"code","metadata":{"id":"WAUUddiLB3UB","executionInfo":{"status":"ok","timestamp":1611777036176,"user_tz":-60,"elapsed":25875,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}}},"source":["import time\n","import datetime\n","import numpy as np\n","\n","\n","def flat_accuracy(preds, labels):\n","  pred_flat = np.argmax(preds, axis = 1).flatten()\n","  labels_flat = labels.flatten()\n","  return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","\n","def get_preds(preds, labels):\n","  return np.argmax(preds, axis = 1).flatten(), labels.flatten()\n","\n","\n","def analyze_preds(preds, labels):\n","  tp = 0\n","  tn = 0\n","  fp = 0\n","  fn = 0\n","  for index, val in enumerate(preds):\n","    if val == 1 and labels[index] == 1:\n","      # true positive\n","      tp += 1\n","    elif val == 1 and labels[index] == 2: # ternary\n","      # true positive\n","      tp += 1\n","    elif val == 0 and labels[index] == 0: \n","      # true negative\n","      tn += 1\n","    elif val == 1 and labels[index] == 0:\n","      # false positive\n","      fp += 1\n","    elif val == 2 and labels[index] == 0: # ternary\n","      # false positive\n","      fp += 1 \n","    elif val == 1 and labels[index] == 2: # ternary\n","      # false positive\n","      fp += 1\n","    elif val == 2 and labels[index] == 1: # ternary\n","      # false positive\n","      fp += 1\n","    else:\n","      # false negative\n","      fn += 1\n","    \n","  return [tp, tn, fp, fn]\n","\n","\n","def format_time(elapsed):\n","  \"\"\" Takes a time in seconds and returns a string in hh:mm:ss\"\"\"\n","  elapsed_rounded = int(round(elapsed))\n","  return str(datetime.timedelta(seconds=elapsed_rounded))\n","\n"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"c71qytpndgJH","executionInfo":{"status":"ok","timestamp":1611777036177,"user_tz":-60,"elapsed":25867,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}}},"source":["#print(train_dataloader)\n","#for step, batch in enumerate(train_dataloader):\n","#  print(batch)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qdos7xKBCPiM"},"source":["**Starting training**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bVBrOXszCSF5","executionInfo":{"status":"ok","timestamp":1611777543872,"user_tz":-60,"elapsed":533541,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}},"outputId":"febbe0b8-453c-432b-f480-b0a757838bec"},"source":["import random\n","import numpy as np\n","from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support, precision_score, recall_score, classification_report\n","\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","training_stats = []\n","\n","final_prec = []\n","final_rec = []\n","final_f1 = []\n","final_conf_matrix = []\n","\n","# Time measuring the training process\n","total_t0 = time.time()\n","\n","# Tracking lists for calculating the average F1 score over the epochs\n","\n","for epoch_i in range(0, epochs):\n","\n","  # ================================\n","  #           TRAINING!\n","  # ================================\n","  print(\"\")\n","  print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","  print('Training...')\n","  final = False\n","  # Checking if the current epoch is the final one\n","  if epoch_i + 1 == epochs:\n","    final = True\n","  # Measuring how long one epoch takes:\n","  t0 = time.time()\n","\n","  # Reset the total loss for this epoch\n","  total_train_loss = 0\n","\n","  # Chaning the mode of the model to train:\n","  model.train()\n","  train_preds = []\n","  train_labels = []\n","\n","  # Tracking lists for the scorers:\n","  train_prec = []\n","  train_rec = []\n","  train_f1 = []\n","\n","  test_prec = []\n","  test_rec = []\n","  test_f1 = []\n","\n","  # For each batch of training data...\n","  for step, batch in enumerate(train_dataloader):\n","\n","    # Progress update every 40 batches ( even bekijken hoe ik dit met abusive data wil doen)\n","    if step % 40 == 0 and not step == 0:\n","      # calculating elapsed time in minutes\n","      elapsed = format_time(time.time() - t0)\n","      # reporting progress\n","      print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","    # Hier nog controleren of de lengte van de batch wel hetzelfde is\n","    #print(len(batch))\n","\n","    # Copying each tensor to the GPU\n","    # Batch contains:\n","    # [0] = input ids\n","    # [1] = attention masks\n","    # [2] = labels\n","\n","    b_input_ids = batch[0].to(device)\n","    b_input_mask = batch[1].to(device)\n","    b_labels = batch[2].to(device)\n","\n","    # Clearing previously calculated gradients\n","    model.zero_grad()\n","\n","    # Performing a forward pass\n","    loss, logits = model(b_input_ids,\n","                         token_type_ids=None,\n","                         attention_mask=b_input_mask,\n","                         labels=b_labels,\n","                         return_dict=False)\n","    \n","  #  print(logits)\n","  \n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    # Accumulate the training loss over all of the batches (for average loss at the end)\n","    total_train_loss += loss.item()\n","    preds, labs = get_preds(logits, label_ids)\n","    train_preds.extend(preds)\n","    train_labels.extend(labs)\n","    # Performing a backward pass to calculate the gradients\n","    loss.backward()\n","\n","    # Clipping the norm of the gradients to 1.0\n","    # This prevents 'exploding gradients'\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","    # Updating the parameters and take a step using the computed gradient\n","    optimizer.step()\n","\n","    # Update the learning rate\n","    scheduler.step()\n","\n","\n","  conf_matrix = confusion_matrix(train_labels, train_preds)\n","  print('Confusion Matrix: ')\n","  print(conf_matrix)\n","  print(\"\\n\")\n","\n","  # binary\n","  #tn, fp, fn, tp = conf_matrix.ravel() # binary\n","  #print('TN: {} | TP: {} | FN: {} | FP: {}'.format(tn, tp, fn, fp)) # binary\n","\n","  # ternary values\n","  FP = conf_matrix.sum(axis=0) - np.diag(conf_matrix) # ternary\n","  FN = conf_matrix.sum(axis=1) - np.diag(conf_matrix) # ternary\n","  TP = np.diag(conf_matrix) # ternary\n","  TN = conf_matrix.sum() - (FP + FN + TP) # ternary\n","  \n","  fp = FP.astype(float) # ternary\n","  fn = FN.astype(float) # ternary\n","  tp = TP.astype(float) # ternary\n","  tn = TN.astype(float) # ternary\n","\n","  print(\"\\n\")\n","  precision = tp / (tp + fp)\n","  recall = tp / (tp + fn)\n","  f_score = 2 * ((precision * recall) / (precision + recall))\n","  print('Precision: ', precision)\n","  print('Recall: ', recall)\n","  print('F1: ', f_score)\n","  print(\"\\n\")\n","  # Calculate the average loss over all of the batches\n","  avg_train_loss = total_train_loss / len(train_dataloader)\n","\n","  # Measure how long this epoch took\n","  training_time = format_time(time.time() - t0)\n","  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","  print(\"  Training epcoh took: {:}\".format(training_time))\n","  print(\"\\n\")\n","\n","  # ========================================\n","  #               Validation\n","  # ========================================\n","  # After the completion of each training epoch, measure our performance on\n","  # our validation set.\n","\n","  print(\"Running Validation\")\n","  print(\"\\n\")\n","  t0 = time.time()\n","\n","  # Put the model into evaluation mode\n","  model.eval()\n","\n","  # Tracking variables\n","  total_eval_accuracy = 0\n","  total_eval_loss = 0\n","  nb_eval_steps = 0\n","  tp = 0\n","  fp = 0\n","  fn = 0\n","  tn = 0\n","  predicted_labels = []\n","  actual_labels = []\n","\n","  # Evaluate data for one epoch\n","  for batch in validation_dataloader:\n","    # Unpacking the validation batch\n","    b_input_ids = batch[0].to(device)\n","    b_input_mask = batch[1].to(device)\n","    b_labels = batch[2].to(device)\n","\n","    # For validation, we don't need to construct the compute graph\n","    with torch.no_grad():\n","\n","      (loss, logits) = model(b_input_ids,\n","                             token_type_ids = None,\n","                             attention_mask = b_input_mask,\n","                             labels = b_labels,\n","                             return_dict=False)\n","    \n","    # Accumulate the validation loss\n","    total_eval_loss += loss.item()\n","\n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Calculating the accuracy for this batch of test sentences\n","    total_eval_accuracy += flat_accuracy(logits, label_ids)\n","    preds, labs = get_preds(logits, label_ids)\n","    predicted_labels.extend(preds)\n","    actual_labels.extend(labs)\n","\n","    results = analyze_preds(preds, labs)\n","    tp += results[0]\n","    fp += results[2]\n","    fn += results[3]\n","    tn += results[1]\n","\n","  \n","  precision = tp / (tp + fp)\n","  recall = tp / (tp + fn)\n","  f_score = 2 * (precision * recall) / (precision + recall)\n","  print('Precision: ', precision)\n","  print('Recall: ', recall)\n","  print('F1: ', f_score)\n","  print(\"\\n\")\n","  #print('Confusion Matrix:') # binary\n","  #conf_matrix = confusion_matrix(actual_labels, predicted_labels) # binary\n","  #print(conf_matrix) # binary\n","  #print('TN: {} | TP: {} | FN: {} | FP: {}'.format(tn, tp, fn, fp)) # binary\n","  avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","  print(classification_report(actual_labels, predicted_labels))\n"," # print('Sklearn Precision Micro ', precision_score(actual_labels, predicted_labels, average='micro'))\n"," # print('Sklearn Precision Macro ', precision_score(actual_labels, predicted_labels, average='macro'))\n"," # print('Sklearn Recall Micro ', recall_score(actual_labels, predicted_labels, average='micro'))\n"," # print('Sklearn Recall Macro ', recall_score(actual_labels, predicted_labels, average='macro'))\n","  #print('Sklrean f1 score (Macro): ', f1_score(actual_labels, predicted_labels, average='macro'))\n","\n","  print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","  print(\"\\n\")\n","  if final:\n","    print('Final epoch has been reached, appending results...')\n","    print(\"\\n\")\n","    final_prec.append(precision)\n","    final_rec.append(recall)\n","    final_f1.append(f_score)\n","    final_conf_matrix = conf_matrix\n","  # Calculate the average loss over all of the batches.\n","  avg_val_loss = total_eval_loss / len(validation_dataloader)\n","  \n","  # Measure how long the validation run took.\n","  validation_time = format_time(time.time() - t0)\n","  \n","  print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","  print(\"  Validation took: {:}\".format(validation_time))\n","  print(\"\\n\")\n","  # Record all statistics from this epoch.\n","  training_stats.append(\n","      {\n","          'epoch': epoch_i + 1,\n","          'Training Loss': avg_train_loss,\n","          'Valid. Loss': avg_val_loss,\n","          'Valid. Accur.': avg_val_accuracy,\n","          'Training Time': training_time,\n","          'Validation Time': validation_time\n","      }\n","  )\n","  \n","#print(\"\")\n","#print(\"Training complete!\")\n","#print(\"\\n\")\n","#print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","#print(\"\\n\")\n","#print('Precision: ', final_prec)\n","#print('Recall: ', final_rec)\n","#print('F1: ', final_f1)\n"],"execution_count":18,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 5 ========\n","Training...\n","  Batch    40  of    179.    Elapsed: 0:00:21.\n","  Batch    80  of    179.    Elapsed: 0:00:42.\n","  Batch   120  of    179.    Elapsed: 0:01:03.\n","  Batch   160  of    179.    Elapsed: 0:01:25.\n","Confusion Matrix: \n","[[4400  164    0]\n"," [ 446  292    0]\n"," [ 330   71    3]]\n","\n","\n","\n","\n","Precision:  [0.85007728 0.5540797  1.        ]\n","Recall:  [0.96406661 0.39566396 0.00742574]\n","F1:  [0.90349076 0.46166008 0.01474201]\n","\n","\n","  Average training loss: 0.48\n","  Training epcoh took: 0:01:35\n","\n","\n","Running Validation\n","\n","\n","Precision:  0.8787878787878788\n","Recall:  0.26605504587155965\n","F1:  0.4084507042253521\n","\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.84      0.99      0.91       439\n","           1       0.88      0.36      0.51        77\n","           2       0.00      0.00      0.00        33\n","\n","    accuracy                           0.85       549\n","   macro avg       0.57      0.45      0.48       549\n","weighted avg       0.80      0.85      0.80       549\n","\n","  Accuracy: 0.84\n","\n","\n","  Validation Loss: 0.40\n","  Validation took: 0:00:03\n","\n","\n","\n","======== Epoch 2 / 5 ========\n","Training...\n","  Batch    40  of    179.    Elapsed: 0:00:22.\n","  Batch    80  of    179.    Elapsed: 0:00:45.\n","  Batch   120  of    179.    Elapsed: 0:01:07.\n","  Batch   160  of    179.    Elapsed: 0:01:29.\n","Confusion Matrix: \n","[[4429  113   22]\n"," [ 180  529   29]\n"," [ 249  102   53]]\n","\n","\n","\n","\n","Precision:  [0.91169205 0.71102151 0.50961538]\n","Recall:  [0.97042068 0.71680217 0.13118812]\n","F1:  [0.9401401  0.71390013 0.20866142]\n","\n","\n","  Average training loss: 0.33\n","  Training epcoh took: 0:01:39\n","\n","\n","Running Validation\n","\n","\n","Precision:  0.7\n","Recall:  0.5833333333333334\n","F1:  0.6363636363636365\n","\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.94      0.92       439\n","           1       0.63      0.68      0.65        77\n","           2       0.20      0.06      0.09        33\n","\n","    accuracy                           0.85       549\n","   macro avg       0.58      0.56      0.56       549\n","weighted avg       0.83      0.85      0.84       549\n","\n","  Accuracy: 0.85\n","\n","\n","  Validation Loss: 0.37\n","  Validation took: 0:00:03\n","\n","\n","\n","======== Epoch 3 / 5 ========\n","Training...\n","  Batch    40  of    179.    Elapsed: 0:00:22.\n","  Batch    80  of    179.    Elapsed: 0:00:44.\n","  Batch   120  of    179.    Elapsed: 0:01:07.\n","  Batch   160  of    179.    Elapsed: 0:01:29.\n","Confusion Matrix: \n","[[4454   58   52]\n"," [  80  628   30]\n"," [ 170   77  157]]\n","\n","\n","\n","\n","Precision:  [0.94685374 0.82306684 0.65690377]\n","Recall:  [0.97589833 0.85094851 0.38861386]\n","F1:  [0.96115667 0.83677548 0.48833593]\n","\n","\n","  Average training loss: 0.22\n","  Training epcoh took: 0:01:39\n","\n","\n","Running Validation\n","\n","\n","Precision:  0.6888888888888889\n","Recall:  0.5688073394495413\n","F1:  0.6231155778894473\n","\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.94      0.92       439\n","           1       0.65      0.68      0.66        77\n","           2       0.41      0.21      0.28        33\n","\n","    accuracy                           0.86       549\n","   macro avg       0.66      0.61      0.62       549\n","weighted avg       0.84      0.86      0.85       549\n","\n","  Accuracy: 0.86\n","\n","\n","  Validation Loss: 0.40\n","  Validation took: 0:00:03\n","\n","\n","\n","======== Epoch 4 / 5 ========\n","Training...\n","  Batch    40  of    179.    Elapsed: 0:00:22.\n","  Batch    80  of    179.    Elapsed: 0:00:44.\n","  Batch   120  of    179.    Elapsed: 0:01:07.\n","  Batch   160  of    179.    Elapsed: 0:01:29.\n","Confusion Matrix: \n","[[4497   24   43]\n"," [  33  665   40]\n"," [  92   53  259]]\n","\n","\n","\n","\n","Precision:  [0.97295543 0.89622642 0.75730994]\n","Recall:  [0.98531989 0.90108401 0.64108911]\n","F1:  [0.97909863 0.89864865 0.69436997]\n","\n","\n","  Average training loss: 0.14\n","  Training epcoh took: 0:01:39\n","\n","\n","Running Validation\n","\n","\n","Precision:  0.5576923076923077\n","Recall:  0.58\n","F1:  0.5686274509803922\n","\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.92      0.93       439\n","           1       0.69      0.66      0.68        77\n","           2       0.32      0.42      0.36        33\n","\n","    accuracy                           0.85       549\n","   macro avg       0.65      0.67      0.66       549\n","weighted avg       0.86      0.85      0.86       549\n","\n","  Accuracy: 0.86\n","\n","\n","  Validation Loss: 0.43\n","  Validation took: 0:00:03\n","\n","\n","\n","======== Epoch 5 / 5 ========\n","Training...\n","  Batch    40  of    179.    Elapsed: 0:00:22.\n","  Batch    80  of    179.    Elapsed: 0:00:44.\n","  Batch   120  of    179.    Elapsed: 0:01:07.\n","  Batch   160  of    179.    Elapsed: 0:01:29.\n","Confusion Matrix: \n","[[4519   18   27]\n"," [  26  686   26]\n"," [  56   40  308]]\n","\n","\n","\n","\n","Precision:  [0.98217779 0.92204301 0.8531856 ]\n","Recall:  [0.99014023 0.9295393  0.76237624]\n","F1:  [0.98614294 0.92577598 0.80522876]\n","\n","\n","  Average training loss: 0.10\n","  Training epcoh took: 0:01:39\n","\n","\n","Running Validation\n","\n","\n","Precision:  0.6506024096385542\n","Recall:  0.5142857142857142\n","F1:  0.5744680851063829\n","\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.95      0.93       439\n","           1       0.74      0.62      0.68        77\n","           2       0.33      0.27      0.30        33\n","\n","    accuracy                           0.86       549\n","   macro avg       0.66      0.61      0.63       549\n","weighted avg       0.85      0.86      0.85       549\n","\n","  Accuracy: 0.86\n","\n","\n","Final epoch has been reached, appending results...\n","\n","\n","  Validation Loss: 0.52\n","  Validation took: 0:00:03\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JvYwqr8rGgKH"},"source":["Running on the test set\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f-vxcPCkCt9V","executionInfo":{"status":"ok","timestamp":1611777555190,"user_tz":-60,"elapsed":544834,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}},"outputId":"34e18950-166b-449d-bbc1-b7b9073bf6dd"},"source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predicted_labels , true_labels = [], []\n","\n","# Predict \n","for batch in test_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, token_type_ids=None, \n","                      attention_mask=b_input_mask,return_dict=False)\n","\n","  logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  preds, labs = get_preds(logits, label_ids)\n","  \n","  # Store predictions and true labels\n","  predicted_labels.extend(preds)\n","  true_labels.extend(labs)\n","\n","print(classification_report(true_labels, predicted_labels))\n","#print('Sklearn Precision Micro ', precision_score(true_labels, predicted_labels, average='micro'))\n","#print('Precision Macro ', precision_score(true_labels, predicted_labels, average='macro'))\n","#print('Sklearn Recall Micro ', recall_score(true_labels, predicted_labels, average='micro'))\n","#print('Sklearn Recall Macro ', recall_score(true_labels, predicted_labels, average='macro'))\n","#print('Sklrean f1 score (Macro): ', f1_score(true_labels, predicted_labels, average='macro'))\n","\n","print('    DONE.')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Predicting labels for 17 test sentences...\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.92      0.87      1264\n","           1       0.74      0.63      0.68       412\n","           2       0.38      0.23      0.28       225\n","\n","    accuracy                           0.78      1901\n","   macro avg       0.65      0.59      0.61      1901\n","weighted avg       0.75      0.78      0.76      1901\n","\n","    DONE.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tDjvNnNpWKka","executionInfo":{"status":"ok","timestamp":1611777555202,"user_tz":-60,"elapsed":544822,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}}},"source":["# save model \n","\n","#model_save_name = 'te+red_extraf:red-mnt_5-classifier.pt'\n","#path = F\"/content/drive/My Drive/Colab Notebooks/models/{model_save_name}\"  \n","#torch.save(model.state_dict(), path)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wn6tcgJIW4pH","executionInfo":{"status":"ok","timestamp":1611777555204,"user_tz":-60,"elapsed":544813,"user":{"displayName":"T. Caselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64","userId":"01725435240943860109"}},"outputId":"538cd9dd-6c25-4be1-9fb0-2f0f0b850e20"},"source":["predictions = list(zip(test, predicted_labels, true_labels))\n","print(predictions[0:3])\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["[('id', 1, 1), ('text', 0, 0), ('user', 1, 1)]\n"],"name":"stdout"}]}]}