{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3211,
     "status": "ok",
     "timestamp": 1639598304786,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "EJFaICXe5Nyn",
    "outputId": "520b494a-537e-48ab-b334-9694cbdc5dc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# connect to upload data\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4120,
     "status": "ok",
     "timestamp": 1639598308896,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "yJI7tRJR5l00",
    "outputId": "e274f6c0-8f07-4ef5-fc13-c00c030c144b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Get the GPU device name:\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name == '/device:GPU:0':\n",
    "  print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "  #raise SystemError('GPU device not found')\n",
    "  print('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6468,
     "status": "ok",
     "timestamp": 1639598315352,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "W3-1rbWh5pei",
    "outputId": "da28402b-e2b4-429b-daa8-c9ebd0a480ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1639598315358,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "FqNO4awT52uK",
    "outputId": "7a8069f4-7c17-4f03-b418-3a3bf00c03d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 6,817\n",
      "\n",
      "Number of dev sentences: 1,205\n",
      "\n",
      "Number of test sentences: 3,270\n",
      "\n",
      "                                                   text  ... target_aggregated\n",
      "0                                     STEM LIJST 13 URL  ...               NaN\n",
      "1     @USER @USER @USER en dan nu achterste voren sp...  ...             GROUP\n",
      "2                    Als je neukt voor geld alle ja URL  ...        INDIVIDUAL\n",
      "3     @USER Ze doen net of ze zo schijnheilig zijn,h...  ...             GROUP\n",
      "4     @USER @USER Nu is het wel zo dat zwarte mensen...  ...             GROUP\n",
      "...                                                 ...  ...               ...\n",
      "6812                              @USER @USER Nee hoor.  ...               NaN\n",
      "6813                      @USER @USER Het is een topper  ...               NaN\n",
      "6814                     Weer een ontzettend leuke klus  ...               NaN\n",
      "6815  Speciaal voor @USER nog maar eens in de herhal...  ...             OTHER\n",
      "6816  Filmmakers naar rechter: 'Staat doet te weinig...  ...               NaN\n",
      "\n",
      "[6817 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "path = '' # ADAPT PATH\n",
    "\n",
    "# manually curate split\n",
    "train = pd.read_csv(path + 'DALC2_train_full.csv', delimiter='\\t', header=0)\n",
    "\n",
    "dev = pd.read_csv(path + 'DALC2_dev_full.csv', delimiter='\\t', header=0)\n",
    "\n",
    "test = pd.read_csv(path + 'DALC2_test_full.csv', delimiter='\\t', header=0)\n",
    "\n",
    "\n",
    "print('Number of training sentences: {:,}\\n'.format(train.shape[0]))\n",
    "print('Number of dev sentences: {:,}\\n'.format(dev.shape[0]))\n",
    "print('Number of test sentences: {:,}\\n'.format(test.shape[0]))\n",
    "#print(train[['text', 'explicitness', 'target']].head())\n",
    "print(train[['text', 'target_aggregated']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1639598315363,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "UEwPioZvHxwX",
    "outputId": "48a44bc6-593a-462f-c10a-2b9ae17d13d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id  ... target_a4\n",
      "1  1267087494941679617  ...       NaN\n",
      "2   838904123558883328  ...       NOT\n",
      "3  1267575421014609920  ...       NaN\n",
      "4  1062112759255576582  ...       NaN\n",
      "5  1266354740021133318  ...       NaN\n",
      "\n",
      "[5 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# drop NaN for TARGET\n",
    "train.dropna(subset=['target_aggregated'], inplace=True)\n",
    "dev.dropna(subset=['target_aggregated'], inplace=True)\n",
    "test.dropna(subset=['target_aggregated'], inplace=True)\n",
    "\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1639598315714,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "XuOFi0Se8laQ",
    "outputId": "30676c4d-6fa0-4ab3-f3a3-4423716325f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting train labels:\n",
      "IND: 1147 | GRP: 705 | OTH: 489 | NOT: 136\n",
      "Formatting dev labels:\n",
      "IND: 191 | GRP: 133 | OTH: 93 | NOT: 22\n",
      "Formatting test labels:\n",
      "IND: 361 | GRP: 244 | OTH: 157 | NOT: 105\n"
     ]
    }
   ],
   "source": [
    "train_labels = train.target_aggregated.values\n",
    "dev_labels = dev.target_aggregated.values\n",
    "test_labels = test.target_aggregated.values\n",
    "\n",
    "# Reformatting the labels binary, 0 = not abusive, 1 = abusive\n",
    "def reformat_labels(labels):\n",
    "  b_labels = []\n",
    "  not_count = 0\n",
    "  individual_count = 0\n",
    "  group_count = 0\n",
    "  other_count = 0\n",
    "  \n",
    "  for label in labels:\n",
    "    if label == 'NOT':\n",
    "      not_count += 1\n",
    "      b_labels.append(0)\n",
    "    elif label == 'INDIVIDUAL':\n",
    "      individual_count += 1\n",
    "      b_labels.append(1)\n",
    "    elif label == \"GROUP\":\n",
    "      group_count += 1\n",
    "      b_labels.append(2)\n",
    "    else:\n",
    "      other_count += 1\n",
    "      b_labels.append(3)\n",
    "\n",
    "\n",
    "  return b_labels, individual_count, group_count, other_count, not_count # ternary\n",
    "\n",
    "# binary\n",
    "print('Formatting train labels:')\n",
    "train_labels, ind_count, group_count, other_cnt, not_count = reformat_labels(train_labels) \n",
    "print('IND: {} | GRP: {} | OTH: {} | NOT: {}'.format(ind_count, group_count, other_cnt, not_count)) \n",
    "\n",
    "print('Formatting dev labels:')\n",
    "dev_labels, ind_count, group_count, other_cnt, not_count = reformat_labels(dev_labels) \n",
    "print('IND: {} | GRP: {} | OTH: {} | NOT: {}'.format(ind_count, group_count, other_cnt, not_count)) \n",
    "\n",
    "print('Formatting test labels:')\n",
    "test_labels, ind_count, group_count, other_cnt, not_count = reformat_labels(test_labels) \n",
    "print('IND: {} | GRP: {} | OTH: {} | NOT: {}'.format(ind_count, group_count, other_cnt, not_count)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3582,
     "status": "ok",
     "timestamp": 1639598319286,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "vZfwKANyPAZU",
    "outputId": "69e96c58-5be8-41e4-f54e-30cfe8d03d5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MENTION van enige zelfreflectie is bij moslims nooit enige spraken het ligt altijd aan die ander de jood of de christenhond', 'MENTION de lafbek wil medicijnen alleen voor zijn eigen', 'ik vind overigens dat protesten over de dood van die zwarte jongen geheel legitiem zijn ik keur alleen het plunderen af en de riots die erdoor werden aangezwengeld door antifa door dat gegeven krijgt de politie mijn steun weer om dit keihard aan te pakken dom dom dom van ze']\n"
     ]
    }
   ],
   "source": [
    "def clean_samples(data):\n",
    "\n",
    "  new_samples = []\n",
    "  #print(data.head())\n",
    "\n",
    "  content = list(data['text'].values)\n",
    "  for tweet_message in content:\n",
    "      tweet_message = tweet_message.lower()\n",
    "      tweet_message = re.sub(r'(@\\w+)','MENTION', tweet_message)\n",
    "      tweet_message = re.sub(r'(https\\S+)','URL', tweet_message)\n",
    "      tweet_message = re.sub(r'[0-9]+', 'NUMBER', tweet_message)\n",
    "      tweet_message = emoji.demojize(tweet_message)\n",
    "      tweet_message = re.sub(r'#', '', tweet_message)\n",
    "      tweet_message = re.sub(r'[(#.,\\/?!@$%^&*)]', '', tweet_message)\n",
    "      new_samples.append(tweet_message)\n",
    "\n",
    "  return new_samples\n",
    "\n",
    "## Formatting other dataframes as well\n",
    "train_clean = clean_samples(train) # list\n",
    "dev_clean = clean_samples(dev) # list\n",
    "test_clean = clean_samples(test) # list\n",
    "\n",
    "print(dev_clean[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 560,
     "status": "ok",
     "timestamp": 1639598319826,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "s5KGiwCl-Ybu",
    "outputId": "4af7f56d-b04e-4086-dbda-1fb363f71d8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "tokenize results : ['Ik', 'liep', 'naar', 'huis', '.', 'Dat', 'deed', 'ik', 'gisteren']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#tokenizer = nltk.data.load('tokenizers/punkt/dutch.pickle')\n",
    "\n",
    "# Tokenize tweet into words\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text, language='dutch')\n",
    "# check the function\n",
    "#sample_text = 'he did not say anything  about what is going to  happen'\n",
    "sample_text = 'Ik liep naar huis. Dat deed ik gisteren'\n",
    "print(\"tokenize results :\", tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3110,
     "status": "ok",
     "timestamp": 1639598322932,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "uJQ7DryhAQWo",
    "outputId": "bf198ce1-29f9-4314-ed22-6745bf6dfb11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "                                               tweet  ...                                    tweet_tokenized\n",
      "0  MENTION tja als ze die pyramide wel in hadden ...  ...  [MENTION, tja, als, ze, die, pyramide, wel, in...\n",
      "1  pvdd en groenlinks spnl pvv eigenschuld en ik ...  ...  [pvdd, en, groenlinks, spnl, pvv, eigenschuld,...\n",
      "2  gewone burgers worden gelijk van twitter geple...  ...  [gewone, burgers, worden, gelijk, van, twitter...\n",
      "3  deze invalide vrouw zou negers hebbben neerges...  ...  [deze, invalide, vrouw, zou, negers, hebbben, ...\n",
      "4  MENTION deze akwasi snapt het zelf niet eens e...  ...  [MENTION, deze, akwasi, snapt, het, zelf, niet...\n",
      "\n",
      "[5 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def text_prepare(text):\n",
    "    text_join = ' '.join([x for x in tokenize(text)])\n",
    "    text_split = [x for x in tokenize(text)]\n",
    "    return text_join, text_split\n",
    "\n",
    "text_train, text_tokenzied_train = [text_prepare(x)[0] for x in train_clean], [text_prepare(x)[1] for x in train_clean]\n",
    "text_dev, text_tokenzied_dev = [text_prepare(x)[0] for x in dev_clean], [text_prepare(x)[1] for x in dev_clean],\n",
    "text_test, text_tokenzied_test = [text_prepare(x)[0] for x in test_clean], [text_prepare(x)[1] for x in test_clean]\n",
    "\n",
    "msg_lenght = [len(x) for x in text_tokenzied_train]\n",
    "MAX_SEQUENCE_LENGTH = max(msg_lenght)\n",
    "\n",
    "print(MAX_SEQUENCE_LENGTH)\n",
    "##print(text_tokenzied_dev)\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_label_enc = le.fit_transform(train_labels)\n",
    "dev_label_enc = le.fit_transform(dev_labels)\n",
    "test_label_enc = le.fit_transform(test_labels)\n",
    "\n",
    "d_train = {'tweet':text_train,'label':train_label_enc}\n",
    "d_dev = {'tweet':text_dev,'label':dev_label_enc}\n",
    "d_test = {'tweet':text_test,'label':test_label_enc}\n",
    "\n",
    "df_train = pd.DataFrame(d_train, columns=['tweet','label'])\n",
    "df_dev = pd.DataFrame(d_dev, columns=['tweet','label'])\n",
    "df_test = pd.DataFrame(d_test, columns=['tweet','label'])\n",
    "\n",
    "# tokenized tweets\n",
    "df_train['tweet_tokenized'] = text_tokenzied_train\n",
    "df_dev['tweet_tokenized'] = text_tokenzied_dev\n",
    "df_test['tweet_tokenized'] = text_tokenzied_test\n",
    "\n",
    "# shuffle entries df\n",
    "df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_dev = df_dev.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(df_train.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1639598322934,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "fzpNdfpCC5TN"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Dense, Embedding, LSTM, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 20363,
     "status": "ok",
     "timestamp": 1639598343288,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "9wVGBuwPDNSz"
   },
   "outputs": [],
   "source": [
    "# downloading the coosto model\n",
    "import requests\n",
    "WE_FILE = 'coosto.bin'\n",
    "\n",
    "def download_word_embeddings():\n",
    "    wordembeddings_url = 'https://github.com/coosto/dutch-word-embeddings/releases/download/v1.0/model.bin'\n",
    "\n",
    "    r = requests.get(wordembeddings_url)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        with open(WE_FILE, 'wb') as wordembeddings_file:\n",
    "            wordembeddings_file.write(\n",
    "                r.content\n",
    "            )\n",
    "    \n",
    "download_word_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12891,
     "status": "ok",
     "timestamp": 1639598356161,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "HbAPX9agEzbx",
    "outputId": "d1558a38-9e74-49a7-be5d-322ae849201c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:summarizer.preprocessing.cleaner:'pattern' package not found; tag filters are not available for English\n",
      "INFO:gensim.models.utils_any2vec:loading projection weights from coosto.bin\n",
      "INFO:gensim.models.utils_any2vec:loaded (250479, 300) matrix from coosto.bin\n"
     ]
    }
   ],
   "source": [
    "# loading coosto/word2vec:\n",
    "import gensim\n",
    "\n",
    "def load_word_embeddings():\n",
    "    return gensim.models.KeyedVectors.load_word2vec_format(WE_FILE, binary=True)\n",
    "\n",
    "word_model = load_word_embeddings()\n",
    "#print(word_model['huis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1639598356162,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "WxOmjP2hNW0X"
   },
   "outputs": [],
   "source": [
    "X_train_text, X_train_tokenized, X_dev_text, X_dev_tokenized, X_test_text, X_test_tokenized, y_train, y_dev, y_test = df_train.tweet, df_train.tweet_tokenized, df_dev.tweet, df_dev.tweet_tokenized, df_test.tweet, df_test.tweet_tokenized, df_train.label, df_dev.label, df_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1639598356167,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "BVAvOSMQIy21"
   },
   "outputs": [],
   "source": [
    "# vocab == COOSTO vocab\n",
    "def _vectorize_data(data, wm):\n",
    "    # turn the tokens into coosto vocab indices\n",
    "    # these will be converted to embeddings in the Embedding layer\n",
    "    vocab = wm.vocab\n",
    "    keys = list(vocab.keys())\n",
    "\n",
    "    final = []\n",
    "    for tweet in data:\n",
    "        final.append([keys.index(word) for word in tweet if vocab.get(word, None) is not None])\n",
    "    return final\n",
    "\n",
    "def vectorize_data(all_tweets, model):\n",
    "    # pad so each message has equal max lenght train set.\n",
    "    return pad_sequences(\n",
    "        sequences=_vectorize_data(all_tweets, model),\n",
    "        maxlen = MAX_SEQUENCE_LENGTH,\n",
    "        padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bilstm_target(embedding_matrix, nclasses=4, MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH, EMBEDDING_DIM=300, dropout=0.5, hidden_layer = 0, lstm_node = 50):\n",
    "    # Initialize a sequential model\n",
    "    model = Sequential()            \n",
    "    # Add embedding layer\n",
    "    model.add(Embedding(\n",
    "                        input_dim = embedding_matrix.shape[0],\n",
    "                        output_dim = embedding_matrix.shape[1], \n",
    "                        input_length = MAX_SEQUENCE_LENGTH,\n",
    "                        weights = [embedding_matrix],\n",
    "                        trainable=False)\n",
    "                        )\n",
    "\n",
    "    # Add hidden layers \n",
    "    for i in range(0,hidden_layer):\n",
    "        # Add a bidirectional lstm layer\n",
    "        model.add(Bidirectional(LSTM(lstm_node, return_sequences=True, recurrent_dropout=0.1)))\n",
    "        # Add a dropout layer after each lstm layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Bidirectional(LSTM(lstm_node, recurrent_dropout=0.1)))\n",
    "    model.add(Dropout(dropout))\n",
    "    # Add the fully connected layer and relu activation\n",
    "    model.add(Dense(64, activation='relu')) # 128 # 64\n",
    "    # Add the output layer with softmax activation ternary\n",
    "    model.add(Dense(nclasses, activation='softmax')) # ternary\n",
    "#    model.add(Dense(nclasses, activation='sigmoid')) #binary\n",
    "\n",
    "\n",
    "    # Compile the model using sparse_categorical_crossentropy\n",
    "    model.compile(loss='sparse_categorical_crossentropy', #sparse_categorical_crossentropy\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "#    # Compile the model using binary_crossentropy\n",
    "#    model.compile(loss='binary_crossentropy', #sparse_categorical_crossentropy\n",
    "#                      optimizer='adam',\n",
    "#                      metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(word_model):\n",
    "    return build_bilstm_target(\n",
    "                        embedding_matrix=word_model.vectors\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9743,
     "status": "ok",
     "timestamp": 1639598365890,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "-LSLprJ2OKV1",
    "outputId": "3dc9de32-2115-4475-eb62-22c6312d0c00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MENTION', 'tja', 'als', 'ze', 'die', 'pyramide', 'wel', 'in', 'hadden', 'gewild', 'hadden', 'ze', 'gisteren', 'wel', 'met', 'de', 'reserves', 'gespeeld', 'raar', 'verhaal', 'MENTION', 'schaam', 'je']\n",
      "[ 25904  16122     10     42   1203    139      1    215    643   1665\n",
      "    778     16     34   4033 143608   9614   3491   4033    815    198\n",
      "  13785      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0]\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "# vectorization \n",
    "df_train_tokenized = df_train[\"tweet_tokenized\"].to_numpy()\n",
    "df_dev_tokenized = df_dev[\"tweet_tokenized\"].to_numpy()\n",
    "df_test_tokenized = df_test[\"tweet_tokenized\"].to_numpy()\n",
    "\n",
    "X_train_ = vectorize_data(df_train_tokenized, word_model)\n",
    "X_dev_ = vectorize_data(df_dev_tokenized, word_model)\n",
    "X_test_ = vectorize_data(df_test_tokenized, word_model)\n",
    "#\n",
    "print(df_train_tokenized[0])\n",
    "print(X_dev_[0])\n",
    "print(len(X_dev_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 620,
     "status": "ok",
     "timestamp": 1639598366496,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "Io7sGApwNJRM",
    "outputId": "eba43066-f3a7-4214-e238-5d5ae8b3d8dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 97, 300)           75143700  \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 100)              140400    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                6464      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 75,290,824\n",
      "Trainable params: 147,124\n",
      "Non-trainable params: 75,143,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# version 2021/12/13 - binary\n",
    "model = build_model(word_model) \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 710622,
     "status": "ok",
     "timestamp": 1639599077099,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "CZC_czJPY9IR",
    "outputId": "7d9a66fb-c374-44fd-c204-4e3f5e9c86d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.1405 - accuracy: 0.5099\n",
      "Epoch 00001: val_loss improved from -inf to 1.00936, saving model to /content/gdrive/MyDrive/Teaching/20 21/IK-BA Thesis/Offensive_Language/models/target_offensive_bilstm/dalc_offensive-target_2.h5\n",
      "78/78 [==============================] - 91s 1s/step - loss: 1.1405 - accuracy: 0.5099 - val_loss: 1.0094 - val_accuracy: 0.5695\n",
      "Epoch 2/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.9457 - accuracy: 0.6217\n",
      "Epoch 00002: val_loss did not improve from 1.00936\n",
      "78/78 [==============================] - 79s 1s/step - loss: 0.9457 - accuracy: 0.6217 - val_loss: 0.9163 - val_accuracy: 0.6173\n",
      "Epoch 3/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.8541 - accuracy: 0.6568\n",
      "Epoch 00003: val_loss did not improve from 1.00936\n",
      "78/78 [==============================] - 79s 1s/step - loss: 0.8541 - accuracy: 0.6568 - val_loss: 0.9269 - val_accuracy: 0.6150\n",
      "Epoch 4/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.7787 - accuracy: 0.7065\n",
      "Epoch 00004: val_loss did not improve from 1.00936\n",
      "78/78 [==============================] - 79s 1s/step - loss: 0.7787 - accuracy: 0.7065 - val_loss: 0.8850 - val_accuracy: 0.6173\n",
      "Epoch 5/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.7226 - accuracy: 0.7319\n",
      "Epoch 00005: val_loss did not improve from 1.00936\n",
      "78/78 [==============================] - 76s 971ms/step - loss: 0.7226 - accuracy: 0.7319 - val_loss: 0.8997 - val_accuracy: 0.6196\n",
      "Epoch 6/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.6437 - accuracy: 0.7558\n",
      "Epoch 00006: val_loss did not improve from 1.00936\n",
      "78/78 [==============================] - 77s 984ms/step - loss: 0.6437 - accuracy: 0.7558 - val_loss: 0.8819 - val_accuracy: 0.6583\n",
      "Epoch 7/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.5889 - accuracy: 0.7796\n",
      "Epoch 00007: val_loss did not improve from 1.00936\n",
      "78/78 [==============================] - 78s 998ms/step - loss: 0.5889 - accuracy: 0.7796 - val_loss: 0.9200 - val_accuracy: 0.6446\n",
      "Epoch 8/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.5129 - accuracy: 0.8115\n",
      "Epoch 00008: val_loss did not improve from 1.00936\n",
      "78/78 [==============================] - 78s 995ms/step - loss: 0.5129 - accuracy: 0.8115 - val_loss: 0.9251 - val_accuracy: 0.6720\n",
      "Epoch 9/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.4496 - accuracy: 0.8337\n",
      "Epoch 00009: val_loss did not improve from 1.00936\n",
      "78/78 [==============================] - 75s 966ms/step - loss: 0.4496 - accuracy: 0.8337 - val_loss: 0.9869 - val_accuracy: 0.6446\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "# fit model into data\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint \n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=3)\n",
    "mc = ModelCheckpoint('path2savemodel', monitor='val_loss', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "history = model.fit(X_train_, y_train,\n",
    "                           validation_data=(X_dev_, y_dev),\n",
    "                           epochs=100,\n",
    "                           batch_size=32,\n",
    "                           verbose=1,\n",
    "                           callbacks=[es,mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3128,
     "status": "ok",
     "timestamp": 1639599080205,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "rHnX7Pxh-ytU",
    "outputId": "9194e1ad-7c59-4f77-94f6-7422ee26987b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      2\n",
      "1      3\n",
      "2      2\n",
      "3      1\n",
      "4      1\n",
      "      ..\n",
      "862    2\n",
      "863    0\n",
      "864    1\n",
      "865    3\n",
      "866    3\n",
      "Name: label, Length: 867, dtype: int64\n",
      "[[2.62873899e-03 1.65285483e-01 8.22428346e-01 9.65747144e-03]\n",
      " [2.55950801e-02 1.27450945e-02 8.04196477e-01 1.57463372e-01]\n",
      " [6.04041945e-03 8.89720954e-03 4.51843917e-01 5.33218443e-01]\n",
      " ...\n",
      " [6.36407232e-04 9.98282671e-01 7.17235089e-04 3.63738858e-04]\n",
      " [4.91250038e-01 3.23871970e-01 1.01935275e-01 8.29427764e-02]\n",
      " [5.17121796e-03 1.59798760e-03 8.82649660e-01 1.10581078e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "print(model.predict(X_test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2653,
     "status": "ok",
     "timestamp": 1639599082852,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "Aniacf-CKL_u",
    "outputId": "ea7ac70c-e819-4a6b-c211-97974e79b124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating Model ... \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3784    0.1333    0.1972       105\n",
      "           1     0.6308    0.8615    0.7283       361\n",
      "           2     0.6296    0.6270    0.6283       244\n",
      "           3     0.5000    0.2994    0.3745       157\n",
      "\n",
      "    accuracy                         0.6055       867\n",
      "   macro avg     0.5347    0.4803    0.4821       867\n",
      "weighted avg     0.5762    0.6055    0.5718       867\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# categorical_cross entropy - ternary\n",
    "print(\"\\n Evaluating Model ... \\n\")\n",
    "predicted = y_predict = np.argmax(model.predict(X_test_), axis=-1)\n",
    "print(metrics.classification_report(y_test, predicted, digits=4))\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "S3gGStu01cp5"
   ],
   "name": "DALC_BiLSTM_offensive-target.ipynb",
   "provenance": [
    {
     "file_id": "1PqwIZvUz4dG4crK6NtPiIAJe4SETyA2B",
     "timestamp": 1638275578736
    },
    {
     "file_id": "https://github.com/robinvandernoord/thesis/blob/master/Thesis_final.ipynb",
     "timestamp": 1638204747243
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
