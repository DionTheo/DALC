{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24584,
     "status": "ok",
     "timestamp": 1639589617207,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "EJFaICXe5Nyn",
    "outputId": "cbc55da9-5400-471d-e3e4-500af5c1d48d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# connect to upload data\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5645,
     "status": "ok",
     "timestamp": 1639589625096,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "yJI7tRJR5l00",
    "outputId": "a8639478-762a-401d-ecc6-298778b14c73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Get the GPU device name:\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name == '/device:GPU:0':\n",
    "  print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "  #raise SystemError('GPU device not found')\n",
    "  print('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3205,
     "status": "ok",
     "timestamp": 1639589639298,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "FqNO4awT52uK",
    "outputId": "fc8bdcf4-e087-4602-cd2f-48190ca9dbef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 6,817\n",
      "\n",
      "Number of dev sentences: 1,205\n",
      "\n",
      "Number of test sentences: 3,270\n",
      "\n",
      "                                                   text  ... target_aggregated\n",
      "0                                     STEM LIJST 13 URL  ...               NaN\n",
      "1     @USER @USER @USER en dan nu achterste voren sp...  ...             GROUP\n",
      "2                    Als je neukt voor geld alle ja URL  ...        INDIVIDUAL\n",
      "3     @USER Ze doen net of ze zo schijnheilig zijn,h...  ...             GROUP\n",
      "4     @USER @USER Nu is het wel zo dat zwarte mensen...  ...             GROUP\n",
      "...                                                 ...  ...               ...\n",
      "6812                              @USER @USER Nee hoor.  ...               NaN\n",
      "6813                      @USER @USER Het is een topper  ...               NaN\n",
      "6814                     Weer een ontzettend leuke klus  ...               NaN\n",
      "6815  Speciaal voor @USER nog maar eens in de herhal...  ...             OTHER\n",
      "6816  Filmmakers naar rechter: 'Staat doet te weinig...  ...               NaN\n",
      "\n",
      "[6817 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import emoji\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#path = '/content/gdrive/MyDrive/Teaching/20 21/IK-BA Thesis/Offensive_Language/data4distribution_aggregated/'\n",
    "path = '' # ADAPT SCRIPT\n",
    "\n",
    "# manually curate split\n",
    "train = pd.read_csv(path + 'DALC2_train_full.csv', delimiter='\\t', header=0)\n",
    "\n",
    "dev = pd.read_csv(path + 'DALC2_dev_full.csv', delimiter='\\t', header=0)\n",
    "\n",
    "test = pd.read_csv(path + 'DALC2_test_full.csv', delimiter='\\t', header=0)\n",
    "\n",
    "\n",
    "print('Number of training sentences: {:,}\\n'.format(train.shape[0]))\n",
    "print('Number of dev sentences: {:,}\\n'.format(dev.shape[0]))\n",
    "print('Number of test sentences: {:,}\\n'.format(test.shape[0]))\n",
    "#print(train[['text', 'explicitness', 'target']].head())\n",
    "#print(train[['text', 'offensive_aggregated', 'target_aggregated']])\n",
    "print(train[['text', 'abusive_offensive_not']]) # READY FOR NEW EXPERIMENTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 559,
     "status": "ok",
     "timestamp": 1639589645818,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "XuOFi0Se8laQ",
    "outputId": "6555e375-1175-4f55-eb5c-4a07fd11c840"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting train labels:\n",
      "EXP: 1407 | IMP: 0 | NOT: 4340\n",
      "Formatting dev labels:\n",
      "EXP: 230 | IMP: 0 | NOT: 766\n",
      "Formatting test labels:\n",
      "EXP: 584 | IMP: 0 | NOT: 2403\n"
     ]
    }
   ],
   "source": [
    "# Placing the sentence and label columns into a list of values\n",
    "\n",
    "#train_labels = train.explicitness.values\n",
    "#dev_labels = dev.explicitness.values\n",
    "#test_labels = test.explicitness.values\n",
    "\n",
    "#train_labels = train.offensive_aggregated.values\n",
    "#dev_labels = dev.offensive_aggregated.values\n",
    "#test_labels = test.offensive_aggregated.values\n",
    "\n",
    "train_labels = train.abusive_offensive_not.values\n",
    "dev_labels = dev.abusive_offensive_not.values\n",
    "test_labels = test.abusive_offensive_not.values\n",
    "\n",
    "\n",
    "# Reformatting the labels binary, 0 = not abusive, 1 = abusive\n",
    "def reformat_labels(labels):\n",
    "  b_labels = []\n",
    "  abusive_count = 0\n",
    "  not_count = 0\n",
    "  offensive_count = 0\n",
    "  \n",
    "  for label in labels:\n",
    "    # binary\n",
    "    #if label == 'NOT':\n",
    "    #  not_count += 1\n",
    "    #  b_labels.append(0)\n",
    "    #else:\n",
    "    #  abusive_count += 1\n",
    "    #  b_labels.append(1)\n",
    "\n",
    "    # ternary\n",
    "    if label == 'NOT':\n",
    "      not_count += 1\n",
    "      b_labels.append(0)\n",
    "    elif label == 'OFFENSE':\n",
    "      offensive_count += 1\n",
    "      b_labels.append(1)\n",
    "    else:\n",
    "      abusive_count += 1\n",
    "      b_labels.append(2)\n",
    "\n",
    "\n",
    "#  return b_labels, abusive_count, not_count # binary\n",
    "  return b_labels, explicit_count, implicit_count, not_count # ternary\n",
    "\n",
    "# binary\n",
    "print('Formatting train labels:')\n",
    "#train_labels, off_count, not_count = reformat_labels(train_labels) # binary\n",
    "train_labels, abusive_count, offensive_count, not_count = reformat_labels(train_labels) # ternary\n",
    "#print('OFF: {} | NOT: {}'.format(off_count, not_count)) # binary\n",
    "#\n",
    "print('ABU: {} | OFF: {} | NOT: {}'.format(abusive_count, offensive_count, not_count)) # ternary\n",
    "\n",
    "print('Formatting dev labels:')\n",
    "#dev_labels, off_count, not_count = reformat_labels(dev_labels)\n",
    "dev_labels, abusive_count, offensive_count, not_count = reformat_labels(dev_labels)\n",
    "#print('OFF: {} | NOT: {}'.format(off_count, not_count))\n",
    "print('ABU: {} | OFF: {} | NOT: {}'.format(offensive_count, offensive_count, not_count))\n",
    "\n",
    "print('Formatting test labels:')\n",
    "#test_labels, off_count, not_count = reformat_labels(test_labels)\n",
    "test_labels, abusive_count, offensive_count, not_count = reformat_labels(test_labels)\n",
    "#print('OFF: {} | NOT: {}'.format(off_count, not_count))\n",
    "print('ABU: {} | OFF: {} | NOT: {}'.format(abusive_count, offensive_count, not_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSFI3fIP94_8"
   },
   "outputs": [],
   "source": [
    "# versione mia\n",
    "#def clean_samples(data):\n",
    "#\n",
    "#  new_samples = []\n",
    "#  #print(data.head())\n",
    "#\n",
    "#  content = list(data['text'].values)\n",
    "#  for tweet_message in content:\n",
    "#      tweet_message = re.sub(r'https.*[^ ]', 'URL', tweet_message)\n",
    "#      tweet_message = re.sub(r'http.*[^ ]', 'URL', tweet_message)\n",
    "#      tweet_message = re.sub(r'@([^ ]*)', 'USER', tweet_message)\n",
    "#      tweet_message = emoji.demojize(tweet_message)\n",
    "#      tweet_message = re.sub(r'(:.*?:)', r' \\1 ', tweet_message)\n",
    "#      tweet_message = re.sub(' +', ' ', tweet_message)\n",
    "#      new_samples.append(tweet_message)\n",
    "#\n",
    "#  return new_samples\n",
    "#\n",
    "## Formatting other dataframes as well\n",
    "#train_clean = clean_samples(train) # list\n",
    "#dev_clean = clean_samples(dev) # list\n",
    "#test_clean = clean_samples(test) # list\n",
    "#\n",
    "#print(dev_clean[0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7889,
     "status": "ok",
     "timestamp": 1639589660827,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "vZfwKANyPAZU",
    "outputId": "a13a526e-d9a3-48ca-dd2a-a67c3d3792ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MENTION heb dat juist bij amsterdammers lmaooo', 'MENTION van enige zelfreflectie is bij moslims nooit enige spraken het ligt altijd aan die ander de jood of de christenhond', 'MENTION de lafbek wil medicijnen alleen voor zijn eigen']\n"
     ]
    }
   ],
   "source": [
    "#version to be used for all experiments LREC 2022\n",
    "def clean_samples(data):\n",
    "\n",
    "  new_samples = []\n",
    "  #print(data.head())\n",
    "\n",
    "  content = list(data['text'].values)\n",
    "  for tweet_message in content:\n",
    "      tweet_message = tweet_message.lower()\n",
    "      tweet_message = re.sub(r'(@\\w+)','MENTION', tweet_message)\n",
    "      tweet_message = re.sub(r'(https\\S+)','URL', tweet_message)\n",
    "      tweet_message = re.sub(r'[0-9]+', 'NUMBER', tweet_message)\n",
    "      tweet_message = emoji.demojize(tweet_message)\n",
    "      tweet_message = re.sub(r'#', '', tweet_message)\n",
    "      tweet_message = re.sub(r'[(#.,\\/?!@$%^&*)]', '', tweet_message)\n",
    "      new_samples.append(tweet_message)\n",
    "\n",
    "  return new_samples\n",
    "\n",
    "## Formatting other dataframes as well\n",
    "train_clean = clean_samples(train) # list\n",
    "dev_clean = clean_samples(dev) # list\n",
    "test_clean = clean_samples(test) # list\n",
    "\n",
    "print(dev_clean[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2159,
     "status": "ok",
     "timestamp": 1639589669022,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "s5KGiwCl-Ybu",
    "outputId": "d01bc7b6-133e-4f26-bfc6-3d6478732eb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "tokenize results : ['Ik', 'liep', 'naar', 'huis', '.', 'Dat', 'deed', 'ik', 'gisteren']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#tokenizer = nltk.data.load('tokenizers/punkt/dutch.pickle')\n",
    "\n",
    "# Tokenize tweet into words\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text, language='dutch')\n",
    "# check the function\n",
    "#sample_text = 'he did not say anything  about what is going to  happen'\n",
    "sample_text = 'Ik liep naar huis. Dat deed ik gisteren'\n",
    "print(\"tokenize results :\", tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9123,
     "status": "ok",
     "timestamp": 1639589680895,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "uJQ7DryhAQWo",
    "outputId": "98ffc884-51f7-4023-8ca1-bf259abe10d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "                                               tweet  ...                                    tweet_tokenized\n",
      "0  MENTION MENTION neehelaas maar is de enige ind...  ...  [MENTION, MENTION, neehelaas, maar, is, de, en...\n",
      "1    MENTION zeker : thumbs_up_light_skin_tone : url  ...  [MENTION, zeker, :, thumbs_up_light_skin_tone,...\n",
      "2           MENTION MENTION owh ik zie een schildpad  ...   [MENTION, MENTION, owh, ik, zie, een, schildpad]\n",
      "3  conducteur van MENTION `` ik wens u vandaag éé...  ...  [conducteur, van, MENTION, ``, ik, wens, u, va...\n",
      "4  kabinet dient noodwet in voor gebruik telecomd...  ...  [kabinet, dient, noodwet, in, voor, gebruik, t...\n",
      "\n",
      "[5 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def text_prepare(text):\n",
    "    text_join = ' '.join([x for x in tokenize(text)])\n",
    "    text_split = [x for x in tokenize(text)]\n",
    "    return text_join, text_split\n",
    "\n",
    "text_train, text_tokenzied_train = [text_prepare(x)[0] for x in train_clean], [text_prepare(x)[1] for x in train_clean]\n",
    "text_dev, text_tokenzied_dev = [text_prepare(x)[0] for x in dev_clean], [text_prepare(x)[1] for x in dev_clean],\n",
    "text_test, text_tokenzied_test = [text_prepare(x)[0] for x in test_clean], [text_prepare(x)[1] for x in test_clean]\n",
    "\n",
    "msg_lenght = [len(x) for x in text_tokenzied_train]\n",
    "MAX_SEQUENCE_LENGTH = max(msg_lenght)\n",
    "\n",
    "print(MAX_SEQUENCE_LENGTH)\n",
    "##print(text_tokenzied_dev)\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_label_enc = le.fit_transform(train_labels)\n",
    "dev_label_enc = le.fit_transform(dev_labels)\n",
    "test_label_enc = le.fit_transform(test_labels)\n",
    "\n",
    "d_train = {'tweet':text_train,'label':train_label_enc}\n",
    "d_dev = {'tweet':text_dev,'label':dev_label_enc}\n",
    "d_test = {'tweet':text_test,'label':test_label_enc}\n",
    "\n",
    "df_train = pd.DataFrame(d_train, columns=['tweet','label'])\n",
    "df_dev = pd.DataFrame(d_dev, columns=['tweet','label'])\n",
    "df_test = pd.DataFrame(d_test, columns=['tweet','label'])\n",
    "\n",
    "# tokenized tweets\n",
    "df_train['tweet_tokenized'] = text_tokenzied_train\n",
    "df_dev['tweet_tokenized'] = text_tokenzied_dev\n",
    "df_test['tweet_tokenized'] = text_tokenzied_test\n",
    "\n",
    "# shuffle entries df\n",
    "df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_dev = df_dev.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(df_train.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1639589683866,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "fzpNdfpCC5TN"
   },
   "outputs": [],
   "source": [
    "# Create BiLSTM model from Keras - https://medium.com/analytics-vidhya/building-a-text-classification-model-using-bilstm-c0548ace26f2\n",
    "from keras.layers import Dropout, Dense, Embedding, LSTM, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 14319,
     "status": "ok",
     "timestamp": 1639589704421,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "9wVGBuwPDNSz"
   },
   "outputs": [],
   "source": [
    "# downloading the coosto model\n",
    "import requests\n",
    "WE_FILE = 'coosto.bin'\n",
    "\n",
    "def download_word_embeddings():\n",
    "    wordembeddings_url = 'https://github.com/coosto/dutch-word-embeddings/releases/download/v1.0/model.bin'\n",
    "\n",
    "    r = requests.get(wordembeddings_url)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        with open(WE_FILE, 'wb') as wordembeddings_file:\n",
    "            wordembeddings_file.write(\n",
    "                r.content\n",
    "            )\n",
    "    \n",
    "download_word_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5544,
     "status": "ok",
     "timestamp": 1639589713559,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "HbAPX9agEzbx",
    "outputId": "bad581f9-493b-44d0-b29b-c0b1901f7d11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:summarizer.preprocessing.cleaner:'pattern' package not found; tag filters are not available for English\n",
      "INFO:gensim.models.utils_any2vec:loading projection weights from coosto.bin\n",
      "INFO:gensim.models.utils_any2vec:loaded (250479, 300) matrix from coosto.bin\n"
     ]
    }
   ],
   "source": [
    "# loading coosto/word2vec:\n",
    "import gensim\n",
    "\n",
    "def load_word_embeddings():\n",
    "    return gensim.models.KeyedVectors.load_word2vec_format(WE_FILE, binary=True)\n",
    "\n",
    "word_model = load_word_embeddings()\n",
    "#print(word_model['huis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1639589716183,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "WxOmjP2hNW0X"
   },
   "outputs": [],
   "source": [
    "#X_train, X_dev, X_test, y_train, y_dev, y_test = df_train.tweet, df_dev.tweet, df_test.tweet, df_train.label, df_dev.label, df_test.label \n",
    "X_train_text, X_train_tokenized, X_dev_text, X_dev_tokenized, X_test_text, X_test_tokenized, y_train, y_dev, y_test = df_train.tweet, df_train.tweet_tokenized, df_dev.tweet, df_dev.tweet_tokenized, df_test.tweet, df_test.tweet_tokenized, df_train.label, df_dev.label, df_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1639589718572,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "BVAvOSMQIy21"
   },
   "outputs": [],
   "source": [
    "# version 2021/12/13 - vectorize; vocab == COOSTO vocab\n",
    "def _vectorize_data(data, wm):\n",
    "    # turn the tokens into coosto vocab indices\n",
    "    # these will be converted to embeddings in the Embedding layer\n",
    "    vocab = wm.vocab\n",
    "    keys = list(vocab.keys())\n",
    "\n",
    "    final = []\n",
    "    for tweet in data:\n",
    "        final.append([keys.index(word) for word in tweet if vocab.get(word, None) is not None])\n",
    "    return final\n",
    "\n",
    "def vectorize_data(all_tweets, model):\n",
    "    # pad so each message has equal max lenght train set.\n",
    "    return pad_sequences(\n",
    "        sequences=_vectorize_data(all_tweets, model),\n",
    "        maxlen = MAX_SEQUENCE_LENGTH,\n",
    "        padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNkf5yhhdcrv"
   },
   "outputs": [],
   "source": [
    "# version 2021/12/08 - vectorize; vocab = train\n",
    "#from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "#vectorizer = TextVectorization(output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "#text_ds = tf.data.Dataset.from_tensor_slices(X_train_text).batch(128)\n",
    "#vectorizer.adapt(text_ds)\n",
    "#voc = vectorizer.get_vocabulary()\n",
    "#word_index = dict(zip(voc, range(len(voc))))\n",
    "#\n",
    "#print(voc[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mW9J0WwrBtC-"
   },
   "outputs": [],
   "source": [
    "# version 2021/12/08 \n",
    "#def build_bilstm(word_index, embeddings_dict, nclasses,  MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH, EMBEDDING_DIM=300, dropout=0.5, hidden_layer = 1, lstm_node = 32):\n",
    "#    # Initialize a sequebtial model\n",
    "#    model = Sequential()\n",
    "    # Make the embedding matrix using the embedding_dict\n",
    "#    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "#    for word, i in word_index.items():\n",
    "#        try:\n",
    "#            embedding_vector = embeddings_dict[word]\n",
    "#        except KeyError:\n",
    "#            pass\n",
    "#            embedding_vector = None\n",
    "#        if embedding_vector is not None:\n",
    "#            # words not found in embedding index will be all-zeros.\n",
    "#            if len(embedding_matrix[i]) != len(embedding_vector):\n",
    "#                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])), \"into shape\", str(len(embedding_vector)), \" Please make sure your EMBEDDING_DIM is equal to embedding_vector file\")\n",
    "#                exit(1)\n",
    "#            embedding_matrix[i] = embedding_vector\n",
    "#            \n",
    "#    # Add embedding layer\n",
    "#    model.add(Embedding(len(word_index) + 1,\n",
    "#                                EMBEDDING_DIM,\n",
    "#                                weights=[embedding_matrix],\n",
    "#                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                                trainable=False)) # --> from True\n",
    "#    # Add hidden layers \n",
    "#    for i in range(0,hidden_layer):\n",
    "#        # Add a bidirectional lstm layer\n",
    "#        model.add(Bidirectional(LSTM(lstm_node, return_sequences=True, recurrent_dropout=0.2)))\n",
    "#        # Add a dropout layer after each lstm layer\n",
    "#        model.add(Dropout(dropout))\n",
    "#    model.add(Bidirectional(LSTM(lstm_node, recurrent_dropout=0.2)))\n",
    "#    model.add(Dropout(dropout))\n",
    "#    # Add the fully connected layer with 256 nurons and relu activation\n",
    "#    model.add(Dense(64, activation='relu')) # 128\n",
    "#    # Add the output layer with softmax activation since we have 2 classes\n",
    "#    model.add(Dense(nclasses, activation='softmax')) #nclasses #activation='softmax'\n",
    "#    # Compile the model using sparse_categorical_crossentropy\n",
    "#    model.compile(loss='sparse_categorical_crossentropy', #sparse_categorical_crossentropy\n",
    "#                      optimizer='adam',\n",
    "#                      metrics=['accuracy'])\n",
    "#    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Amf64-vOCQID"
   },
   "outputs": [],
   "source": [
    "# version 2021/12/13\n",
    "def build_bilstm_binary(embedding_matrix, nclasses=2, MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH, EMBEDDING_DIM=300, dropout=0.5, hidden_layer = 0, lstm_node = 32):\n",
    "    # Initialize a sequential model\n",
    "    model = Sequential()            \n",
    "    # Add embedding layer\n",
    "    model.add(Embedding(\n",
    "                        input_dim = embedding_matrix.shape[0],\n",
    "                        output_dim = embedding_matrix.shape[1], \n",
    "                        input_length = MAX_SEQUENCE_LENGTH,\n",
    "                        weights = [embedding_matrix],\n",
    "                        trainable=False)\n",
    "                        )\n",
    "\n",
    "    # Add hidden layers \n",
    "    for i in range(0,hidden_layer):\n",
    "        # Add a bidirectional lstm layer\n",
    "        model.add(Bidirectional(LSTM(lstm_node, return_sequences=True, recurrent_dropout=0.1)))\n",
    "        # Add a dropout layer after each lstm layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Bidirectional(LSTM(lstm_node, recurrent_dropout=0.1)))\n",
    "    model.add(Dropout(dropout))\n",
    "    # Add the fully connected layer and relu activation\n",
    "    model.add(Dense(16, activation='relu')) # 128 # 64\n",
    "    # Add the output layer with softmax activation ternary\n",
    "    model.add(Dense(nclasses, activation='softmax')) # ternary\n",
    "#    model.add(Dense(nclasses, activation='sigmoid')) #binary\n",
    "\n",
    "\n",
    "    # Compile the model using sparse_categorical_crossentropy\n",
    "    model.compile(loss='sparse_categorical_crossentropy', #sparse_categorical_crossentropy\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "#    # Compile the model using binary_crossentropy\n",
    "#    model.compile(loss='binary_crossentropy', #sparse_categorical_crossentropy\n",
    "#                      optimizer='adam',\n",
    "#                      metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_binary(word_model):\n",
    "    return build_bilstm_binary(\n",
    "                        embedding_matrix=word_model.vectors\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1639589732210,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "dIurvP5mM3lk"
   },
   "outputs": [],
   "source": [
    "def build_bilstm_ternary(embedding_matrix, nclasses=3, MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH, EMBEDDING_DIM=300, dropout=0.5, hidden_layer = 0, lstm_node = 32):\n",
    "    # Initialize a sequential model\n",
    "    model = Sequential()            \n",
    "    # Add embedding layer\n",
    "    model.add(Embedding(\n",
    "                        input_dim = embedding_matrix.shape[0],\n",
    "                        output_dim = embedding_matrix.shape[1], \n",
    "                        input_length = MAX_SEQUENCE_LENGTH,\n",
    "                        weights = [embedding_matrix],\n",
    "                        trainable=False)\n",
    "                        )\n",
    "\n",
    "    # Add hidden layers \n",
    "    for i in range(0,hidden_layer):\n",
    "        # Add a bidirectional lstm layer\n",
    "        model.add(Bidirectional(LSTM(lstm_node, return_sequences=True, recurrent_dropout=0.3)))\n",
    "        # Add a dropout layer after each lstm layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Bidirectional(LSTM(lstm_node, recurrent_dropout=0.3)))\n",
    "    model.add(Dropout(dropout))\n",
    "    # Add the fully connected layer and relu activation\n",
    "    model.add(Dense(16, activation='relu')) # 128 # 64\n",
    "    # Add the output layer with softmax activation ternary\n",
    "    model.add(Dense(nclasses, activation='softmax')) # ternary\n",
    "#    model.add(Dense(nclasses, activation='sigmoid')) #binary\n",
    "\n",
    "\n",
    "    # Compile the model using sparse_categorical_crossentropy\n",
    "    model.compile(loss='sparse_categorical_crossentropy', #sparse_categorical_crossentropy\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "#    # Compile the model using binary_crossentropy\n",
    "#    model.compile(loss='binary_crossentropy', #sparse_categorical_crossentropy\n",
    "#                      optimizer='adam',\n",
    "#                      metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_ternary(word_model):\n",
    "    return build_bilstm_ternary(\n",
    "                        embedding_matrix=word_model.vectors\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24110,
     "status": "ok",
     "timestamp": 1639589763266,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "-LSLprJ2OKV1",
    "outputId": "36d8b13a-130e-4969-a502-5b9e8dec9793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MENTION', 'MENTION', 'neehelaas', 'maar', 'is', 'de', 'enige', 'inde', 'milieu', 'straat', 'op', 'het', 'winkelcentrum']\n",
      "[38100 33787   316  8811    11  1035 12958    17   191     8     1  7583\n",
      "  3558     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "# vectorization - version 2021/12/13\n",
    "\n",
    "df_train_tokenized = df_train[\"tweet_tokenized\"].to_numpy()\n",
    "df_dev_tokenized = df_dev[\"tweet_tokenized\"].to_numpy()\n",
    "df_test_tokenized = df_test[\"tweet_tokenized\"].to_numpy()\n",
    "\n",
    "X_train_ = vectorize_data(df_train_tokenized, word_model)\n",
    "X_dev_ = vectorize_data(df_dev_tokenized, word_model)\n",
    "X_test_ = vectorize_data(df_test_tokenized, word_model)\n",
    "#\n",
    "print(df_train_tokenized[0])\n",
    "print(X_dev_[0])\n",
    "print(len(X_dev_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtbwVz5d94NI"
   },
   "outputs": [],
   "source": [
    "# version 2021/12/08\n",
    "# vectorize and and pass data to model - embedding check is done in build_model()\n",
    "\n",
    "#X_train_ = vectorizer(np.array([[s] for s in X_train_text])).numpy()\n",
    "#X_dev_ = vectorizer(np.array([[s] for s in X_dev_text])).numpy()\n",
    "#X_test_ = vectorizer(np.array([[s] for s in X_test_text])).numpy()\n",
    "#print(X_train_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHkuX-_aDpkE"
   },
   "outputs": [],
   "source": [
    "# version 2021/12/08\n",
    "#model = build_bilstm(word_index, word_model, 2) # binary\n",
    "#model = build_bilstm(word_index, word_model, 3) # ternary\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRUq_AF9UrIX"
   },
   "outputs": [],
   "source": [
    "# version 2021/12/13 - binary\n",
    "#model = build_model_binary(word_model) \n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1127,
     "status": "ok",
     "timestamp": 1639589771005,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "Io7sGApwNJRM",
    "outputId": "5ef7e436-3dd0-4fca-bfc3-5320dd336f3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 123, 300)          75143700  \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               85248     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                1040      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 75,230,039\n",
      "Trainable params: 86,339\n",
      "Non-trainable params: 75,143,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# version 2021/12/13 - ternary\n",
    "model = build_model_ternary(word_model) \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1720299,
     "status": "ok",
     "timestamp": 1639591577213,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "CZC_czJPY9IR",
    "outputId": "67a45294-61e7-4e52-a73e-c43abd49c13a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "214/214 [==============================] - ETA: 0s - loss: 0.7999 - accuracy: 0.6658\n",
      "Epoch 00001: val_loss improved from -inf to 0.64744, saving model to /content/gdrive/MyDrive/Teaching/20 21/IK-BA Thesis/Offensive_Language/models/offensive_ternary_bilstm/dalc_offensive_ternary_3.h5\n",
      "214/214 [==============================] - 295s 1s/step - loss: 0.7999 - accuracy: 0.6658 - val_loss: 0.6474 - val_accuracy: 0.7145\n",
      "Epoch 2/100\n",
      "214/214 [==============================] - ETA: 0s - loss: 0.6270 - accuracy: 0.7467\n",
      "Epoch 00002: val_loss did not improve from 0.64744\n",
      "214/214 [==============================] - 241s 1s/step - loss: 0.6270 - accuracy: 0.7467 - val_loss: 0.5852 - val_accuracy: 0.7610\n",
      "Epoch 3/100\n",
      "214/214 [==============================] - ETA: 0s - loss: 0.5564 - accuracy: 0.7836\n",
      "Epoch 00003: val_loss did not improve from 0.64744\n",
      "214/214 [==============================] - 239s 1s/step - loss: 0.5564 - accuracy: 0.7836 - val_loss: 0.5659 - val_accuracy: 0.7751\n",
      "Epoch 4/100\n",
      "214/214 [==============================] - ETA: 0s - loss: 0.5103 - accuracy: 0.8043\n",
      "Epoch 00004: val_loss did not improve from 0.64744\n",
      "214/214 [==============================] - 236s 1s/step - loss: 0.5103 - accuracy: 0.8043 - val_loss: 0.5611 - val_accuracy: 0.7668\n",
      "Epoch 5/100\n",
      "214/214 [==============================] - ETA: 0s - loss: 0.4804 - accuracy: 0.8193\n",
      "Epoch 00005: val_loss did not improve from 0.64744\n",
      "214/214 [==============================] - 237s 1s/step - loss: 0.4804 - accuracy: 0.8193 - val_loss: 0.5640 - val_accuracy: 0.7710\n",
      "Epoch 6/100\n",
      "214/214 [==============================] - ETA: 0s - loss: 0.4490 - accuracy: 0.8269\n",
      "Epoch 00006: val_loss did not improve from 0.64744\n",
      "214/214 [==============================] - 236s 1s/step - loss: 0.4490 - accuracy: 0.8269 - val_loss: 0.5869 - val_accuracy: 0.7801\n",
      "Epoch 7/100\n",
      "214/214 [==============================] - ETA: 0s - loss: 0.4192 - accuracy: 0.8397\n",
      "Epoch 00007: val_loss did not improve from 0.64744\n",
      "214/214 [==============================] - 235s 1s/step - loss: 0.4192 - accuracy: 0.8397 - val_loss: 0.5849 - val_accuracy: 0.7925\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "# fit model into data\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint \n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=3)\n",
    "#mc = ModelCheckpoint('/content/gdrive/MyDrive/Teaching/20 21/IK-BA Thesis/Offensive_Language/models/offensive_binary_bilstm/dalc_offensive_binary_3.h5', monitor='val_loss', mode='max', verbose=1, save_best_only=True)\n",
    "mc = ModelCheckpoint('/content/gdrive/MyDrive/Teaching/20 21/IK-BA Thesis/Offensive_Language/models/offensive_ternary_bilstm/dalc_offensive_ternary_3.h5', monitor='val_loss', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "history = model.fit(X_train_, y_train,\n",
    "                           validation_data=(X_dev_, y_dev),\n",
    "                           epochs=100,\n",
    "                           batch_size=32,\n",
    "                           verbose=1,\n",
    "                           callbacks=[es,mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57547,
     "status": "ok",
     "timestamp": 1639591634699,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "rHnX7Pxh-ytU",
    "outputId": "41242227-c0c0-4d5b-83a7-4b98d3966f51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       2\n",
      "4       1\n",
      "       ..\n",
      "3265    1\n",
      "3266    1\n",
      "3267    2\n",
      "3268    0\n",
      "3269    0\n",
      "Name: label, Length: 3270, dtype: int64\n",
      "[[9.5608330e-01 1.0632416e-02 3.3284292e-02]\n",
      " [9.9573129e-01 2.0256834e-03 2.2429735e-03]\n",
      " [9.9315226e-01 2.2087852e-03 4.6389434e-03]\n",
      " ...\n",
      " [5.4409379e-01 8.0070838e-02 3.7583539e-01]\n",
      " [9.9760336e-01 8.7211351e-04 1.5246532e-03]\n",
      " [7.0249754e-01 7.3420890e-02 2.2408158e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "print(model.predict(X_test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqzvX-nsvLez"
   },
   "outputs": [],
   "source": [
    "# binary cross entropy - binary\n",
    "#print(\"\\n Evaluating Model ... \\n\")\n",
    "#predicted = []\n",
    "#y_predict = model.predict(X_test_)\n",
    "#for elem in y_predict:\n",
    "#  if elem > 0.5:\n",
    "#    predicted.append(1)\n",
    "#  else:\n",
    "#    predicted.append(0)\n",
    "#\n",
    "#print(metrics.classification_report(y_test, predicted, digits=4))\n",
    "#print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10544,
     "status": "ok",
     "timestamp": 1639591645216,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "Aniacf-CKL_u",
    "outputId": "e3defc9e-8d37-452b-b551-7b1132797e0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating Model ... \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8843    0.9447    0.9135      2403\n",
      "           1     0.7523    0.7021    0.7263       584\n",
      "           2     0.4241    0.2367    0.3039       283\n",
      "\n",
      "    accuracy                         0.8401      3270\n",
      "   macro avg     0.6869    0.6278    0.6479      3270\n",
      "weighted avg     0.8209    0.8401    0.8273      3270\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# categorical_cross entropy - ternary\n",
    "print(\"\\n Evaluating Model ... \\n\")\n",
    "predicted = y_predict = np.argmax(model.predict(X_test_), axis=-1)\n",
    "print(metrics.classification_report(y_test, predicted, digits=4))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugu1d2tV_rkx"
   },
   "outputs": [],
   "source": [
    "#def prepare_model_input(X_train, X_test, MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH):\n",
    " #   np.random.seed(7)\n",
    "    #text = np.concatenate((X_train, X_test), axis=0)\n",
    "    #print(text)\n",
    "    #text = np.array(text)\n",
    "    \n",
    " #   text = np.array(X_train)\n",
    "    #tokenizer = Tokenizer(num_words=MAX_NB_WORDS) MAX_NB_WORDS=75000\n",
    " #   tokenizer = Tokenizer()\n",
    " #   tokenizer.fit_on_texts(text)\n",
    " #   # pickle.dump(tokenizer, open('text_tokenizer.pkl', 'wb'))\n",
    " #   # Uncomment above line to save the tokenizer as .pkl file \n",
    " #   sequences = tokenizer.texts_to_sequences(text)\n",
    " #   word_index = tokenizer.word_index\n",
    " #   text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    " #   print('Found %s unique tokens.' % len(word_index))\n",
    " #   indices = np.arange(text.shape[0])\n",
    " #   # np.random.shuffle(indices)\n",
    " #   text = text[indices]\n",
    " #   print(text.shape)\n",
    " #   X_train_coosto = text[0:len(X_train), ]\n",
    " #   X_test_coosto = text[len(X_train):, ]\n",
    " #   word_model = load_word_embeddings()\n",
    "##    print('Total %s word vectors.' % len(embeddings_dict))\n",
    " #   return (X_train_coosto, X_test_coosto, word_index, word_model)\n",
    "### Check function\n",
    "#x_train_sample = [\"Gaan we nu op die tour? Wist u dat kanker patiënten op hold worden gezet, voor de CORONA hetze. Belachelijk. Het gaat voor mij de verkeerde richting uit, had gehoopt u aan het denken te zetten.. Ik wens u het beste.\", \"Daarom heb ik besloten dat ik ten laatste op 1 juli 2021 mijn normale leven zal hernemen, ongeacht wat u op dat moment gepast vindt. Er zijn kleine landen die er in slagen om dagelijks 150000 vaccinaties uit te voeren. Wat houdt u tegen? Uw ongekende mogelijkheden?\"]\n",
    "#x_test_sample = [\"Fijne dag goedgelovigen.\", \"Respectloos tegenover de Coronadoden en de zorg.\"]\n",
    "#X_train_coosto_s, X_test_coosto_s, word_index_s, embeddings_dict_s = prepare_model_input(x_train_sample, x_test_sample, MAX_SEQUENCE_LENGTH)\n",
    "#print(\"\\n X_train_emb \\n \", X_train_coosto_s)\n",
    "#print(\"\\n X_test_emb \\n \", X_test_coosto_s)\n",
    "#print(\"\\n Word index of the word gezet is : \", word_index_s[\"gezet\"])\n",
    "#print(\"\\n Embedding for the word zorg \\n \\n\", embeddings_dict_s[\"zorg\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "S3gGStu01cp5"
   ],
   "name": "DALC_BiLSTM_offensive.ipynb",
   "provenance": [
    {
     "file_id": "1PqwIZvUz4dG4crK6NtPiIAJe4SETyA2B",
     "timestamp": 1638275578736
    },
    {
     "file_id": "https://github.com/robinvandernoord/thesis/blob/master/Thesis_final.ipynb",
     "timestamp": 1638204747243
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
