{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23764,
     "status": "ok",
     "timestamp": 1639515329729,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "EJFaICXe5Nyn",
    "outputId": "33af4b57-8591-4818-dd13-234844b066d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# connect to upload data\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2247,
     "status": "ok",
     "timestamp": 1639515340702,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "FqNO4awT52uK",
    "outputId": "a5beb5b6-3df2-483b-af85-c2f5c492342f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 6,817\n",
      "\n",
      "Number of dev sentences: 1,205\n",
      "\n",
      "Number of test sentences: 3,270\n",
      "\n",
      "                                                   text  ... target_aggregated\n",
      "0                                     STEM LIJST 13 URL  ...               NaN\n",
      "1     @USER @USER @USER en dan nu achterste voren sp...  ...             GROUP\n",
      "2                    Als je neukt voor geld alle ja URL  ...        INDIVIDUAL\n",
      "3     @USER Ze doen net of ze zo schijnheilig zijn,h...  ...             GROUP\n",
      "4     @USER @USER Nu is het wel zo dat zwarte mensen...  ...             GROUP\n",
      "...                                                 ...  ...               ...\n",
      "6812                              @USER @USER Nee hoor.  ...               NaN\n",
      "6813                      @USER @USER Het is een topper  ...               NaN\n",
      "6814                     Weer een ontzettend leuke klus  ...               NaN\n",
      "6815  Speciaal voor @USER nog maar eens in de herhal...  ...             OTHER\n",
      "6816  Filmmakers naar rechter: 'Staat doet te weinig...  ...               NaN\n",
      "\n",
      "[6817 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import emoji\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#path = '/content/gdrive/MyDrive/Teaching/20 21/IK-BA Thesis/Offensive_Language/data4distribution_aggregated/'\n",
    "path = '' # ADAPT SCRIPT\n",
    "\n",
    "# manually curate split\n",
    "train = pd.read_csv(path + 'DALC2_train_full.csv', delimiter='\\t', header=0)\n",
    "\n",
    "dev = pd.read_csv(path + 'DALC2_dev_full.csv', delimiter='\\t', header=0)\n",
    "\n",
    "test = pd.read_csv(path + 'DALC2_test_full.csv', delimiter='\\t', header=0)\n",
    "\n",
    "\n",
    "print('Number of training sentences: {:,}\\n'.format(train.shape[0]))\n",
    "print('Number of dev sentences: {:,}\\n'.format(dev.shape[0]))\n",
    "print('Number of test sentences: {:,}\\n'.format(test.shape[0]))\n",
    "#print(train[['text', 'explicitness', 'target']].head())\n",
    "#print(train[['text', 'offensive_aggregated', 'target_aggregated']])\n",
    "print(train[['text', 'abusive_offensive_not']]) # READY FOR NEW EXPERIMENTS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1639515340703,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "XuOFi0Se8laQ",
    "outputId": "822196aa-52c0-4cb7-9682-9be0cf283304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting train labels:\n",
      "EXP: 1407 | IMP: 0 | NOT: 4340\n",
      "Formatting dev labels:\n",
      "EXP: 230 | IMP: 0 | NOT: 766\n",
      "Formatting test labels:\n",
      "EXP: 584 | IMP: 0 | NOT: 2403\n"
     ]
    }
   ],
   "source": [
    "# Placing the sentence and label columns into a list of values\n",
    "\n",
    "#train_labels = train.explicitness.values\n",
    "#dev_labels = dev.explicitness.values\n",
    "#test_labels = test.explicitness.values\n",
    "\n",
    "#train_labels = train.offensive_aggregated.values\n",
    "#dev_labels = dev.offensive_aggregated.values\n",
    "#test_labels = test.offensive_aggregated.values\n",
    "\n",
    "train_labels = train.abusive_offensive_not.values\n",
    "dev_labels = dev.abusive_offensive_not.values\n",
    "test_labels = test.abusive_offensive_not.values\n",
    "\n",
    "\n",
    "# Reformatting the labels binary, 0 = not abusive, 1 = abusive\n",
    "def reformat_labels(labels):\n",
    "  b_labels = []\n",
    "  abusive_count = 0\n",
    "  not_count = 0\n",
    "  offensive_count = 0\n",
    "  \n",
    "  for label in labels:\n",
    "    # binary\n",
    "    #if label == 'NOT':\n",
    "    #  not_count += 1\n",
    "    #  b_labels.append(0)\n",
    "    #else:\n",
    "    #  abusive_count += 1\n",
    "    #  b_labels.append(1)\n",
    "\n",
    "    # ternary\n",
    "    if label == 'NOT':\n",
    "      not_count += 1\n",
    "      b_labels.append(0)\n",
    "    elif label == 'OFFENSE':\n",
    "      offensive_count += 1\n",
    "      b_labels.append(1)\n",
    "    else:\n",
    "      abusive_count += 1\n",
    "      b_labels.append(2)\n",
    "\n",
    "\n",
    "#  return b_labels, abusive_count, not_count # binary\n",
    "  return b_labels, explicit_count, implicit_count, not_count # ternary\n",
    "\n",
    "# binary\n",
    "print('Formatting train labels:')\n",
    "#train_labels, off_count, not_count = reformat_labels(train_labels) # binary\n",
    "train_labels, abusive_count, offensive_count, not_count = reformat_labels(train_labels) # ternary\n",
    "#print('OFF: {} | NOT: {}'.format(off_count, not_count)) # binary\n",
    "#\n",
    "print('ABU: {} | OFF: {} | NOT: {}'.format(abusive_count, offensive_count, not_count)) # ternary\n",
    "\n",
    "print('Formatting dev labels:')\n",
    "#dev_labels, off_count, not_count = reformat_labels(dev_labels)\n",
    "dev_labels, abusive_count, offensive_count, not_count = reformat_labels(dev_labels)\n",
    "#print('OFF: {} | NOT: {}'.format(off_count, not_count))\n",
    "print('ABU: {} | OFF: {} | NOT: {}'.format(offensive_count, offensive_count, not_count))\n",
    "\n",
    "print('Formatting test labels:')\n",
    "#test_labels, off_count, not_count = reformat_labels(test_labels)\n",
    "test_labels, abusive_count, offensive_count, not_count = reformat_labels(test_labels)\n",
    "#print('OFF: {} | NOT: {}'.format(off_count, not_count))\n",
    "print('ABU: {} | OFF: {} | NOT: {}'.format(abusive_count, offensive_count, not_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1639515340704,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "qSFI3fIP94_8"
   },
   "outputs": [],
   "source": [
    "# versione mia\n",
    "#def clean_samples(data):\n",
    "#\n",
    "#  new_samples = []\n",
    "#  #print(data.head())\n",
    "#\n",
    "#  content = list(data['text'].values)\n",
    "#  for tweet_message in content:\n",
    "#      tweet_message = re.sub(r'https.*[^ ]', 'URL', tweet_message)\n",
    "#      tweet_message = re.sub(r'http.*[^ ]', 'URL', tweet_message)\n",
    "#      tweet_message = re.sub(r'@([^ ]*)', 'USER', tweet_message)\n",
    "#      tweet_message = emoji.demojize(tweet_message)\n",
    "#      tweet_message = re.sub(r'(:.*?:)', r' \\1 ', tweet_message)\n",
    "#      tweet_message = re.sub(' +', ' ', tweet_message)\n",
    "#      new_samples.append(tweet_message)\n",
    "#\n",
    "#  return new_samples\n",
    "#\n",
    "## Formatting other dataframes as well\n",
    "#train_clean = clean_samples(train) # list\n",
    "#dev_clean = clean_samples(dev) # list\n",
    "#test_clean = clean_samples(test) # list\n",
    "#\n",
    "#print(dev_clean[0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11432,
     "status": "ok",
     "timestamp": 1639515352130,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "vZfwKANyPAZU",
    "outputId": "3fbb983e-de29-45d4-e7e7-333b12c3bb0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MENTION heb dat juist bij amsterdammers lmaooo', 'MENTION van enige zelfreflectie is bij moslims nooit enige spraken het ligt altijd aan die ander de jood of de christenhond', 'MENTION de lafbek wil medicijnen alleen voor zijn eigen']\n"
     ]
    }
   ],
   "source": [
    "#version Victor - to be used for all experiments\n",
    "def clean_samples(data):\n",
    "\n",
    "  new_samples = []\n",
    "  #print(data.head())\n",
    "\n",
    "  content = list(data['text'].values)\n",
    "  for tweet_message in content:\n",
    "      tweet_message = tweet_message.lower()\n",
    "      tweet_message = re.sub(r'(@\\w+)','MENTION', tweet_message)\n",
    "      tweet_message = re.sub(r'(https\\S+)','URL', tweet_message)\n",
    "      tweet_message = re.sub(r'[0-9]+', 'NUMBER', tweet_message)\n",
    "      tweet_message = emoji.demojize(tweet_message)\n",
    "      tweet_message = re.sub(r'#', '', tweet_message)\n",
    "      tweet_message = re.sub(r'[(#.,\\/?!@$%^&*)]', '', tweet_message)\n",
    "      new_samples.append(tweet_message)\n",
    "\n",
    "  return new_samples\n",
    "\n",
    "## Formatting other dataframes as well\n",
    "train_clean = clean_samples(train) # list\n",
    "dev_clean = clean_samples(dev) # list\n",
    "test_clean = clean_samples(test) # list\n",
    "\n",
    "print(dev_clean[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1502,
     "status": "ok",
     "timestamp": 1639515353623,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "s5KGiwCl-Ybu",
    "outputId": "fd7c189b-26e6-4219-87f3-21ffb06f4403"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "tokenize results : ['Ik', 'liep', 'naar', 'huis', '.', 'Dat', 'deed', 'ik', 'gisteren']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#tokenizer = nltk.data.load('tokenizers/punkt/dutch.pickle')\n",
    "\n",
    "# Tokenize tweet into words\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text, language='dutch')\n",
    "# check the function\n",
    "#sample_text = 'he did not say anything  about what is going to  happen'\n",
    "sample_text = 'Ik liep naar huis. Dat deed ik gisteren'\n",
    "print(\"tokenize results :\", tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1639515353625,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "B3llOpGC-6qB"
   },
   "outputs": [],
   "source": [
    "# version 2021/12/07\n",
    "#def text_prepare(text):\n",
    "#    text = ' '.join([x for x in tokenize(text)])\n",
    "#    return text\n",
    "\n",
    "#text_tokenzied_train = [text_prepare(x) for x in train_clean]\n",
    "#text_tokenzied_dev = [text_prepare(x) for x in dev_clean]\n",
    "#text_tokenzied_test = [text_prepare(x) for x in test_clean]\n",
    "\n",
    "#msg_lenght = [len(x.split()) for x in text_tokenzied_train]\n",
    "#MAX_SEQUENCE_LENGTH = max(msg_lenght)\n",
    "\n",
    "#le = LabelEncoder()\n",
    "#train_label_enc = le.fit_transform(train_labels)\n",
    "#dev_label_enc = le.fit_transform(dev_labels)\n",
    "#test_label_enc = le.fit_transform(test_labels)\n",
    "\n",
    "#d_train = {'tweet':text_tokenzied_train,'label':train_label_enc}\n",
    "#d_dev = {'tweet':text_tokenzied_dev,'label':dev_label_enc}\n",
    "#d_test = {'tweet':text_tokenzied_test,'label':test_label_enc}\n",
    "\n",
    "#df_train = pd.DataFrame(d_train, columns=['tweet','label'])\n",
    "#df_dev = pd.DataFrame(d_dev, columns=['tweet','label'])\n",
    "#df_test = pd.DataFrame(d_test, columns=['tweet','label'])\n",
    "\n",
    "# shuffle entries df\n",
    "#df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "#df_dev = df_dev.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "#df_test = df_test.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#print(df_train.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7347,
     "status": "ok",
     "timestamp": 1639515360967,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "uJQ7DryhAQWo",
    "outputId": "7f43b335-70ef-40c3-cc15-71dc56e29945"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "                                               tweet  ...                                    tweet_tokenized\n",
      "0  MENTION MENTION neehelaas maar is de enige ind...  ...  [MENTION, MENTION, neehelaas, maar, is, de, en...\n",
      "1    MENTION zeker : thumbs_up_light_skin_tone : url  ...  [MENTION, zeker, :, thumbs_up_light_skin_tone,...\n",
      "2           MENTION MENTION owh ik zie een schildpad  ...   [MENTION, MENTION, owh, ik, zie, een, schildpad]\n",
      "3  conducteur van MENTION `` ik wens u vandaag éé...  ...  [conducteur, van, MENTION, ``, ik, wens, u, va...\n",
      "4  kabinet dient noodwet in voor gebruik telecomd...  ...  [kabinet, dient, noodwet, in, voor, gebruik, t...\n",
      "\n",
      "[5 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# version 2021/12/08\n",
    "def text_prepare(text):\n",
    "    text_join = ' '.join([x for x in tokenize(text)])\n",
    "    text_split = [x for x in tokenize(text)]\n",
    "    return text_join, text_split\n",
    "\n",
    "text_train, text_tokenzied_train = [text_prepare(x)[0] for x in train_clean], [text_prepare(x)[1] for x in train_clean]\n",
    "text_dev, text_tokenzied_dev = [text_prepare(x)[0] for x in dev_clean], [text_prepare(x)[1] for x in dev_clean],\n",
    "text_test, text_tokenzied_test = [text_prepare(x)[0] for x in test_clean], [text_prepare(x)[1] for x in test_clean]\n",
    "\n",
    "msg_lenght = [len(x) for x in text_tokenzied_train]\n",
    "MAX_SEQUENCE_LENGTH = max(msg_lenght)\n",
    "\n",
    "print(MAX_SEQUENCE_LENGTH)\n",
    "#print(text_tokenzied_dev)\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_label_enc = le.fit_transform(train_labels)\n",
    "dev_label_enc = le.fit_transform(dev_labels)\n",
    "test_label_enc = le.fit_transform(test_labels)\n",
    "\n",
    "d_train = {'tweet':text_train,'label':train_label_enc}\n",
    "d_dev = {'tweet':text_dev,'label':dev_label_enc}\n",
    "d_test = {'tweet':text_test,'label':test_label_enc}\n",
    "\n",
    "df_train = pd.DataFrame(d_train, columns=['tweet','label'])\n",
    "df_dev = pd.DataFrame(d_dev, columns=['tweet','label'])\n",
    "df_test = pd.DataFrame(d_test, columns=['tweet','label'])\n",
    "\n",
    "# tokenized tweets\n",
    "df_train['tweet_tokenized'] = text_tokenzied_train\n",
    "df_dev['tweet_tokenized'] = text_tokenzied_dev\n",
    "df_test['tweet_tokenized'] = text_tokenzied_test\n",
    "\n",
    "# shuffle entries df\n",
    "df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_dev = df_dev.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(df_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1639515360968,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "fzpNdfpCC5TN",
    "outputId": "e90d5e60-3608-463f-a3ad-eed685158197"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Create SVM model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1639515360968,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "WxOmjP2hNW0X"
   },
   "outputs": [],
   "source": [
    "#X_train, X_dev, X_test, y_train, y_dev, y_test = df_train.tweet, df_dev.tweet, df_test.tweet, df_train.label, df_dev.label, df_test.label \n",
    "X_train_text, X_train_tokenized, X_dev_text, X_dev_tokenized, X_test_text, X_test_tokenized, y_train, y_dev, y_test = df_train.tweet, df_train.tweet_tokenized, df_dev.tweet, df_dev.tweet_tokenized, df_test.tweet, df_test.tweet_tokenized, df_train.label, df_dev.label, df_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1639515360969,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "3XB9L3rapD3A"
   },
   "outputs": [],
   "source": [
    "def train(Xtrain, Ytrain, x_test, y_gold):\n",
    "\n",
    "    # Train and evaluate\n",
    "\n",
    "    svm_classifier = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "\n",
    "    # unweighted word uni and bigrams\n",
    "    count_word = TfidfVectorizer(ngram_range=(1, 2), stop_words=stopwords.words('dutch'))\n",
    "    count_char = TfidfVectorizer(analyzer='char', ngram_range=(2, 5))\n",
    "\n",
    "    text_features = FeatureUnion([('word', count_word),\n",
    "                                  ('char', count_char),\n",
    "                                  ])\n",
    "\n",
    "    pipeline_svm = Pipeline([(\"features\", text_features), (\"svm\", svm_classifier)])\n",
    "\n",
    "    pipeline_svm.fit(Xtrain, Ytrain)\n",
    "\n",
    "    print('Predicting...')\n",
    "    Yguess = pipeline_svm.predict(x_test) # dev or test data\n",
    "    print(\"SVM scores\")\n",
    "    print(metrics.classification_report(y_gold, Yguess, digits=4))\n",
    "    print(metrics.confusion_matrix(y_gold, Yguess))\n",
    "\n",
    "    dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "    dummy_clf.fit(Xtrain, Ytrain)\n",
    "    Y_guess_dummy = dummy_clf.predict(x_test)\n",
    "    print(\"Dummy classifier scores\")\n",
    "    print(metrics.classification_report(y_gold, Y_guess_dummy, digits=4))\n",
    "    print(metrics.confusion_matrix(y_gold, Y_guess_dummy))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244229,
     "status": "ok",
     "timestamp": 1639515605192,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "B4okWxKXp6gs",
    "outputId": "1c6b7c60-1c8f-4b74-b7fc-5c497c329ae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "SVM scores\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8206    0.9367    0.8749      2403\n",
      "           1     0.7108    0.3955    0.5083       584\n",
      "           2     0.2970    0.2120    0.2474       283\n",
      "\n",
      "    accuracy                         0.7774      3270\n",
      "   macro avg     0.6095    0.5148    0.5435      3270\n",
      "weighted avg     0.7557    0.7774    0.7551      3270\n",
      "\n",
      "[[2251   61   91]\n",
      " [ 302  231   51]\n",
      " [ 190   33   60]]\n",
      "Dummy classifier scores\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7349    1.0000    0.8472      2403\n",
      "           1     0.0000    0.0000    0.0000       584\n",
      "           2     0.0000    0.0000    0.0000       283\n",
      "\n",
      "    accuracy                         0.7349      3270\n",
      "   macro avg     0.2450    0.3333    0.2824      3270\n",
      "weighted avg     0.5400    0.7349    0.6226      3270\n",
      "\n",
      "[[2403    0    0]\n",
      " [ 584    0    0]\n",
      " [ 283    0    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "train(X_train_text,y_train,X_test_text,y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "S3gGStu01cp5"
   ],
   "name": "DALC_offensive_SVM_v1.ipynb",
   "provenance": [
    {
     "file_id": "1bbrTUZwq8o3MiUZRcfDnz7eeX--wpgGl",
     "timestamp": 1639140437964
    },
    {
     "file_id": "1PqwIZvUz4dG4crK6NtPiIAJe4SETyA2B",
     "timestamp": 1638275578736
    },
    {
     "file_id": "https://github.com/robinvandernoord/thesis/blob/master/Thesis_final.ipynb",
     "timestamp": 1638204747243
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
