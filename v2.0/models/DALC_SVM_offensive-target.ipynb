{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1597,
     "status": "ok",
     "timestamp": 1639588468838,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "EJFaICXe5Nyn",
    "outputId": "09ab8e48-6727-435e-82d8-0d638ef57c75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# connect to upload data\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3334,
     "status": "ok",
     "timestamp": 1639588472169,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "yJI7tRJR5l00",
    "outputId": "fa7a4f4d-8602-48f6-c869-62a3dc8b176a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Get the GPU device name:\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name == '/device:GPU:0':\n",
    "  print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "  #raise SystemError('GPU device not found')\n",
    "  print('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2744,
     "status": "ok",
     "timestamp": 1639588474906,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "W3-1rbWh5pei",
    "outputId": "af490d69-c16e-47ec-df62-c3f368278a2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1639588595294,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "FqNO4awT52uK",
    "outputId": "ec04e23b-52f0-4b5d-a263-166d2570b167"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 6,817\n",
      "\n",
      "Number of dev sentences: 1,205\n",
      "\n",
      "Number of test sentences: 3,270\n",
      "\n",
      "                                                   text  ... target_aggregated\n",
      "0                                     STEM LIJST 13 URL  ...               NaN\n",
      "1     @USER @USER @USER en dan nu achterste voren sp...  ...             GROUP\n",
      "2                    Als je neukt voor geld alle ja URL  ...        INDIVIDUAL\n",
      "3     @USER Ze doen net of ze zo schijnheilig zijn,h...  ...             GROUP\n",
      "4     @USER @USER Nu is het wel zo dat zwarte mensen...  ...             GROUP\n",
      "...                                                 ...  ...               ...\n",
      "6812                              @USER @USER Nee hoor.  ...               NaN\n",
      "6813                      @USER @USER Het is een topper  ...               NaN\n",
      "6814                     Weer een ontzettend leuke klus  ...               NaN\n",
      "6815  Speciaal voor @USER nog maar eens in de herhal...  ...             OTHER\n",
      "6816  Filmmakers naar rechter: 'Staat doet te weinig...  ...               NaN\n",
      "\n",
      "[6817 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "path = '' # ADAPT PATH\n",
    "\n",
    "# manually curate split\n",
    "train = pd.read_csv(path + 'DALC2_train_full.csv', delimiter='\\t', header=0)\n",
    "\n",
    "dev = pd.read_csv(path + 'DALC2_dev_full.csv', delimiter='\\t', header=0)\n",
    "\n",
    "test = pd.read_csv(path + 'DALC2_test_full.csv', delimiter='\\t', header=0)\n",
    "\n",
    "\n",
    "print('Number of training sentences: {:,}\\n'.format(train.shape[0]))\n",
    "print('Number of dev sentences: {:,}\\n'.format(dev.shape[0]))\n",
    "print('Number of test sentences: {:,}\\n'.format(test.shape[0]))\n",
    "#print(train[['text', 'explicitness', 'target']].head())\n",
    "print(train[['text', 'target_aggregated']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 326,
     "status": "ok",
     "timestamp": 1639588633544,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "AHmZLR_ZCtVl",
    "outputId": "ee9a1f07-6e69-4689-dcf5-1e42d335f073"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id  ... target_a4\n",
      "1  1267087494941679617  ...       NaN\n",
      "2   838904123558883328  ...       NOT\n",
      "3  1267575421014609920  ...       NaN\n",
      "4  1062112759255576582  ...       NaN\n",
      "5  1266354740021133318  ...       NaN\n",
      "\n",
      "[5 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# drop NaN for TARGET\n",
    "train.dropna(subset=['target_aggregated'], inplace=True)\n",
    "dev.dropna(subset=['target_aggregated'], inplace=True)\n",
    "test.dropna(subset=['target_aggregated'], inplace=True)\n",
    "\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1639588641617,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "XuOFi0Se8laQ",
    "outputId": "26b87745-850d-4e96-9c58-eed53094143d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting train labels:\n",
      "IND: 1147 | GRP: 705 | OTH: 489 | NOT: 136\n",
      "Formatting dev labels:\n",
      "IND: 191 | GRP: 133 | OTH: 93 | NOT: 22\n",
      "Formatting test labels:\n",
      "IND: 361 | GRP: 244 | OTH: 157 | NOT: 105\n"
     ]
    }
   ],
   "source": [
    "# Placing the sentence and label columns into a list of values\n",
    "\n",
    "train_labels = train.target_aggregated.values\n",
    "dev_labels = dev.target_aggregated.values\n",
    "test_labels = test.target_aggregated.values\n",
    "\n",
    "# Reformatting the labels binary, 0 = not abusive, 1 = abusive\n",
    "def reformat_labels(labels):\n",
    "  b_labels = []\n",
    "  not_count = 0\n",
    "  individual_count = 0\n",
    "  group_count = 0\n",
    "  other_count = 0\n",
    "  \n",
    "  for label in labels:\n",
    "    if label == 'NOT':\n",
    "      not_count += 1\n",
    "      b_labels.append(0)\n",
    "    elif label == 'INDIVIDUAL':\n",
    "      individual_count += 1\n",
    "      b_labels.append(1)\n",
    "    elif label == \"GROUP\":\n",
    "      group_count += 1\n",
    "      b_labels.append(2)\n",
    "    else:\n",
    "      other_count += 1\n",
    "      b_labels.append(3)\n",
    "\n",
    "\n",
    "  return b_labels, individual_count, group_count, other_count, not_count # ternary\n",
    "\n",
    "# binary\n",
    "print('Formatting train labels:')\n",
    "train_labels, ind_count, group_count, other_cnt, not_count = reformat_labels(train_labels) \n",
    "print('IND: {} | GRP: {} | OTH: {} | NOT: {}'.format(ind_count, group_count, other_cnt, not_count)) \n",
    "\n",
    "print('Formatting dev labels:')\n",
    "dev_labels, ind_count, group_count, other_cnt, not_count = reformat_labels(dev_labels) \n",
    "print('IND: {} | GRP: {} | OTH: {} | NOT: {}'.format(ind_count, group_count, other_cnt, not_count)) \n",
    "\n",
    "print('Formatting test labels:')\n",
    "test_labels, ind_count, group_count, other_cnt, not_count = reformat_labels(test_labels) \n",
    "print('IND: {} | GRP: {} | OTH: {} | NOT: {}'.format(ind_count, group_count, other_cnt, not_count)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4523,
     "status": "ok",
     "timestamp": 1639588655884,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "vZfwKANyPAZU",
    "outputId": "34316547-9c02-43d0-c54c-9d128d46b0df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MENTION van enige zelfreflectie is bij moslims nooit enige spraken het ligt altijd aan die ander de jood of de christenhond', 'MENTION de lafbek wil medicijnen alleen voor zijn eigen', 'ik vind overigens dat protesten over de dood van die zwarte jongen geheel legitiem zijn ik keur alleen het plunderen af en de riots die erdoor werden aangezwengeld door antifa door dat gegeven krijgt de politie mijn steun weer om dit keihard aan te pakken dom dom dom van ze']\n"
     ]
    }
   ],
   "source": [
    "def clean_samples(data):\n",
    "\n",
    "  new_samples = []\n",
    "  #print(data.head())\n",
    "\n",
    "  content = list(data['text'].values)\n",
    "  for tweet_message in content:\n",
    "      tweet_message = tweet_message.lower()\n",
    "      tweet_message = re.sub(r'(@\\w+)','MENTION', tweet_message)\n",
    "      tweet_message = re.sub(r'(https\\S+)','URL', tweet_message)\n",
    "      tweet_message = re.sub(r'[0-9]+', 'NUMBER', tweet_message)\n",
    "      tweet_message = emoji.demojize(tweet_message)\n",
    "      tweet_message = re.sub(r'#', '', tweet_message)\n",
    "      tweet_message = re.sub(r'[(#.,\\/?!@$%^&*)]', '', tweet_message)\n",
    "      new_samples.append(tweet_message)\n",
    "\n",
    "  return new_samples\n",
    "\n",
    "## Formatting other dataframes as well\n",
    "train_clean = clean_samples(train) # list\n",
    "dev_clean = clean_samples(dev) # list\n",
    "test_clean = clean_samples(test) # list\n",
    "\n",
    "print(dev_clean[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1676,
     "status": "ok",
     "timestamp": 1639588657555,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "s5KGiwCl-Ybu",
    "outputId": "a4a976d0-e164-4cdc-de2d-a5cbe1f8caa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "tokenize results : ['Ik', 'liep', 'naar', 'huis', '.', 'Dat', 'deed', 'ik', 'gisteren']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#tokenizer = nltk.data.load('tokenizers/punkt/dutch.pickle')\n",
    "\n",
    "# Tokenize tweet into words\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text, language='dutch')\n",
    "# check the function\n",
    "#sample_text = 'he did not say anything  about what is going to  happen'\n",
    "sample_text = 'Ik liep naar huis. Dat deed ik gisteren'\n",
    "print(\"tokenize results :\", tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2945,
     "status": "ok",
     "timestamp": 1639588662292,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "uJQ7DryhAQWo",
    "outputId": "1d6b30e3-9137-4c13-e358-9d80a1517933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "                                               tweet  ...                                    tweet_tokenized\n",
      "0  MENTION tja als ze die pyramide wel in hadden ...  ...  [MENTION, tja, als, ze, die, pyramide, wel, in...\n",
      "1  pvdd en groenlinks spnl pvv eigenschuld en ik ...  ...  [pvdd, en, groenlinks, spnl, pvv, eigenschuld,...\n",
      "2  gewone burgers worden gelijk van twitter geple...  ...  [gewone, burgers, worden, gelijk, van, twitter...\n",
      "3  deze invalide vrouw zou negers hebbben neerges...  ...  [deze, invalide, vrouw, zou, negers, hebbben, ...\n",
      "4  MENTION deze akwasi snapt het zelf niet eens e...  ...  [MENTION, deze, akwasi, snapt, het, zelf, niet...\n",
      "\n",
      "[5 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# version 2021/12/08\n",
    "def text_prepare(text):\n",
    "    text_join = ' '.join([x for x in tokenize(text)])\n",
    "    text_split = [x for x in tokenize(text)]\n",
    "    return text_join, text_split\n",
    "\n",
    "text_train, text_tokenzied_train = [text_prepare(x)[0] for x in train_clean], [text_prepare(x)[1] for x in train_clean]\n",
    "text_dev, text_tokenzied_dev = [text_prepare(x)[0] for x in dev_clean], [text_prepare(x)[1] for x in dev_clean],\n",
    "text_test, text_tokenzied_test = [text_prepare(x)[0] for x in test_clean], [text_prepare(x)[1] for x in test_clean]\n",
    "\n",
    "msg_lenght = [len(x) for x in text_tokenzied_train]\n",
    "MAX_SEQUENCE_LENGTH = max(msg_lenght)\n",
    "\n",
    "print(MAX_SEQUENCE_LENGTH)\n",
    "#print(text_tokenzied_dev)\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_label_enc = le.fit_transform(train_labels)\n",
    "dev_label_enc = le.fit_transform(dev_labels)\n",
    "test_label_enc = le.fit_transform(test_labels)\n",
    "\n",
    "d_train = {'tweet':text_train,'label':train_label_enc}\n",
    "d_dev = {'tweet':text_dev,'label':dev_label_enc}\n",
    "d_test = {'tweet':text_test,'label':test_label_enc}\n",
    "\n",
    "df_train = pd.DataFrame(d_train, columns=['tweet','label'])\n",
    "df_dev = pd.DataFrame(d_dev, columns=['tweet','label'])\n",
    "df_test = pd.DataFrame(d_test, columns=['tweet','label'])\n",
    "\n",
    "# tokenized tweets\n",
    "df_train['tweet_tokenized'] = text_tokenzied_train\n",
    "df_dev['tweet_tokenized'] = text_tokenzied_dev\n",
    "df_test['tweet_tokenized'] = text_tokenzied_test\n",
    "\n",
    "# shuffle entries df\n",
    "df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_dev = df_dev.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(df_train.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 352,
     "status": "ok",
     "timestamp": 1639588683650,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "fzpNdfpCC5TN",
    "outputId": "d6fee6df-7e6b-424b-c139-704004ccebf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Create SVM model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1639588687553,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "WxOmjP2hNW0X"
   },
   "outputs": [],
   "source": [
    "X_train_text, X_train_tokenized, X_dev_text, X_dev_tokenized, X_test_text, X_test_tokenized, y_train, y_dev, y_test = df_train.tweet, df_train.tweet_tokenized, df_dev.tweet, df_dev.tweet_tokenized, df_test.tweet, df_test.tweet_tokenized, df_train.label, df_dev.label, df_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1639588689815,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "3XB9L3rapD3A"
   },
   "outputs": [],
   "source": [
    "def train(Xtrain, Ytrain, x_test, y_gold):\n",
    "\n",
    "    # Train and evaluate\n",
    "\n",
    "    svm_classifier = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "\n",
    "    # unweighted word uni and bigrams\n",
    "    count_word = TfidfVectorizer(ngram_range=(1, 2), stop_words=stopwords.words('dutch'))\n",
    "    count_char = TfidfVectorizer(analyzer='char', ngram_range=(2, 5))\n",
    "\n",
    "    text_features = FeatureUnion([('word', count_word),\n",
    "                                  ('char', count_char),\n",
    "                                  ])\n",
    "\n",
    "    pipeline_svm = Pipeline([(\"features\", text_features), (\"svm\", svm_classifier)])\n",
    "\n",
    "    pipeline_svm.fit(Xtrain, Ytrain)\n",
    "\n",
    "    print('Predicting...')\n",
    "    Yguess = pipeline_svm.predict(x_test) # dev or test data\n",
    "    print(\"SVM scores\")\n",
    "    print(metrics.classification_report(y_gold, Yguess, digits=4))\n",
    "    print(metrics.confusion_matrix(y_gold, Yguess))\n",
    "\n",
    "    dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "    dummy_clf.fit(Xtrain, Ytrain)\n",
    "    Y_guess_dummy = dummy_clf.predict(x_test)\n",
    "    print(\"Dummy classifier scores\")\n",
    "    print(metrics.classification_report(y_gold, Y_guess_dummy, digits=4))\n",
    "    print(metrics.confusion_matrix(y_gold, Y_guess_dummy))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44470,
     "status": "ok",
     "timestamp": 1639588736612,
     "user": {
      "displayName": "T. Caselli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDehpIq_AO6gfnyTX-gnpKGQShCOMugA6LtiCgSA=s64",
      "userId": "01725435240943860109"
     },
     "user_tz": -60
    },
    "id": "B4okWxKXp6gs",
    "outputId": "1f79d529-19c8-4d40-ecab-28fdddf24a77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "SVM scores\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.1143    0.1951       105\n",
      "           1     0.5876    0.8920    0.7085       361\n",
      "           2     0.6313    0.5615    0.5944       244\n",
      "           3     0.5357    0.2866    0.3734       157\n",
      "\n",
      "    accuracy                         0.5952       867\n",
      "   macro avg     0.6053    0.4636    0.4678       867\n",
      "weighted avg     0.6001    0.5952    0.5535       867\n",
      "\n",
      "[[ 12  68  13  12]\n",
      " [  4 322  26   9]\n",
      " [  1  88 137  18]\n",
      " [  1  70  41  45]]\n",
      "Dummy classifier scores\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       105\n",
      "           1     0.4164    1.0000    0.5879       361\n",
      "           2     0.0000    0.0000    0.0000       244\n",
      "           3     0.0000    0.0000    0.0000       157\n",
      "\n",
      "    accuracy                         0.4164       867\n",
      "   macro avg     0.1041    0.2500    0.1470       867\n",
      "weighted avg     0.1734    0.4164    0.2448       867\n",
      "\n",
      "[[  0 105   0   0]\n",
      " [  0 361   0   0]\n",
      " [  0 244   0   0]\n",
      " [  0 157   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "train(X_train_text,y_train,X_test_text,y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "S3gGStu01cp5"
   ],
   "name": "DALC_offensive-target_SVM_v1.ipynb",
   "provenance": [
    {
     "file_id": "1bbrTUZwq8o3MiUZRcfDnz7eeX--wpgGl",
     "timestamp": 1639140437964
    },
    {
     "file_id": "1PqwIZvUz4dG4crK6NtPiIAJe4SETyA2B",
     "timestamp": 1638275578736
    },
    {
     "file_id": "https://github.com/robinvandernoord/thesis/blob/master/Thesis_final.ipynb",
     "timestamp": 1638204747243
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
